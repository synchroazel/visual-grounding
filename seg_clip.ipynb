{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Import necessary packages and set correct device\n",
    "\n",
    "import clip\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import slic, watershed\n",
    "from skimage.util import img_as_float\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.ops import box_iou\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.refcocog import RefCOCOg, RefCOCOgSample\n",
    "from modules.yoloclip import display_preds\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # CUDA GPU\n",
    "    print(\"[INFO] Using GPU.\")\n",
    "elif torch.has_mps:\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "    print(\"[INFO] Using MPS.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"[INFO] No GPU found, using CPU instead.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-12T21:35:04.062948Z",
     "end_time": "2023-05-12T21:35:14.968719Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Import RefCOCOg dataset and its train/val/test splits\n",
    "\n",
    "# data_path = \"/media/dmmp/vid+backup/Data/refcocog\"\n",
    "data_path = \"dataset/refcocog\"\n",
    "\n",
    "dataset = RefCOCOg(ds_path=data_path)\n",
    "\n",
    "train_ds = RefCOCOg(ds_path=data_path, split='train')\n",
    "val_ds = RefCOCOg(ds_path=data_path, split='val')\n",
    "test_ds = RefCOCOg(ds_path=data_path, split='test')\n",
    "\n",
    "# keep only a toy portion of each split\n",
    "# keep = 0.1\n",
    "# dataset, _ = random_split(dataset, [int(keep * len(dataset)), len(dataset) - int(keep * len(dataset))])\n",
    "# train_ds, _ = random_split(train_ds, [int(keep * len(train_ds)), len(train_ds) - int(keep * len(train_ds))])\n",
    "# val_ds, _ = random_split(val_ds, [int(keep * len(val_ds)), len(val_ds) - int(keep * len(val_ds))])\n",
    "# test_ds, _ = random_split(test_ds, [int(keep * len(test_ds)), len(test_ds) - int(keep * len(test_ds))])\n",
    "\n",
    "print(f\"Dataset Size: {len(dataset)}\\n\")\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Val size:   {len(val_ds)}\")\n",
    "print(f\"Test size:  {len(test_ds)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-12T21:35:14.969789Z",
     "end_time": "2023-05-12T21:35:26.740571Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Utility function to downsample a map\n",
    "\n",
    "def downsample_map(map, factor):\n",
    "    # number of blocks in each dimension\n",
    "    blocks_h = map.shape[0] // factor\n",
    "    blocks_w = map.shape[1] // factor\n",
    "\n",
    "    # reshape the original matrix into blocks\n",
    "    blocks = map[:blocks_h * factor, :blocks_w * factor].reshape(blocks_h, factor, blocks_w, factor)\n",
    "\n",
    "    # calculate the average of each block\n",
    "    averages = blocks.mean(axis=(1, 3))\n",
    "\n",
    "    return averages\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-12T21:35:26.741216Z",
     "end_time": "2023-05-12T21:35:26.778809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@ Utility function to perform best bounding box search\n",
    "\n",
    "def find_best_bbox(heatmap, show=False):\n",
    "    heatmap = heatmap.astype(np.uint8)\n",
    "\n",
    "    # Threshold the image to create a binary mask\n",
    "    ret, mask = cv2.threshold(heatmap, heatmap.min(), heatmap.max(), cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the binary mask\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # print(f\"[INFO] {len(contours)} detections.\")\n",
    "\n",
    "    best_score = 0\n",
    "    best_bbox = None\n",
    "\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        score = heatmap[y:y + h, x:x + w].sum()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_bbox = (x, y, w, h)\n",
    "\n",
    "    if show:\n",
    "        # Display the map with the best bbox\n",
    "        x, y, w, h = best_bbox\n",
    "        rect = plt.Rectangle((x, y), w, h, linewidth=1, edgecolor=(0, 1, 0), facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.imshow(map)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return best_bbox\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-12T21:35:26.778926Z",
     "end_time": "2023-05-12T21:35:26.809900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@ Definition of Segmentation + CLIP visual grounding pipeline\n",
    "\n",
    "class ClipSeg:\n",
    "\n",
    "    def __init__(self, methods, n_segments=None, q=0.95, d=16, device=device, quiet=False):\n",
    "\n",
    "        self.methods = list(methods)\n",
    "        self.clip_model, self.clip_prep = clip.load(\"ViT-L/14\", device=device)\n",
    "        self.n_segments = n_segments\n",
    "        self.q = q\n",
    "        self.d = d\n",
    "        self.device = device\n",
    "        self.quiet = quiet\n",
    "\n",
    "        supported_methods = [\"s\", \"w\"]\n",
    "\n",
    "        for method in self.methods:\n",
    "            if method not in supported_methods:\n",
    "                raise ValueError(f\"Method `{method}` not supported. Supported methods are: {supported_methods}\")\n",
    "\n",
    "    def _compute_hmap(self, np_image, prompt, method, masks):\n",
    "\n",
    "        hmaps = list()\n",
    "\n",
    "        for i, n in enumerate(masks):\n",
    "\n",
    "            if method == \"s\":  # slic\n",
    "                segments = slic(np_image, n_segments=n, compactness=10, sigma=1)\n",
    "            elif method == \"w\":  # watershed\n",
    "                segments = watershed(sobel(rgb2gray(np_image)), markers=n, compactness=0.001)\n",
    "\n",
    "            regions = regionprops(segments)\n",
    "\n",
    "            prompt_tkn = clip.tokenize(prompt).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prompt_enc = self.clip_model.encode_text(prompt_tkn)\n",
    "\n",
    "            images_encs = list()\n",
    "\n",
    "            pbar = tqdm(regions, position=0, leave=False) if not self.quiet else regions\n",
    "\n",
    "            for region in pbar:\n",
    "\n",
    "                if not self.quiet:\n",
    "                    pbar.set_description(f\"[INFO] method \\\"{method.upper()}\\\" \"\n",
    "                                         f\"| mask {i + 1}/{len(self.n_segments)} \"\n",
    "                                         f\"| computing CLIP scores\",\n",
    "                                         refresh=True)\n",
    "\n",
    "                rect = region.bbox\n",
    "                rect = (rect[1], rect[0], rect[3], rect[2])\n",
    "\n",
    "                sub_image = sample.img.crop(rect)\n",
    "                sub_image = self.clip_prep(sub_image).unsqueeze(0)\n",
    "                sub_image = sub_image.to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    image_enc = self.clip_model.encode_image(sub_image)\n",
    "\n",
    "                images_encs.append(image_enc)\n",
    "\n",
    "            images_encs = torch.cat(images_encs, dim=0)\n",
    "\n",
    "            scores = prompt_enc @ images_encs.T\n",
    "\n",
    "            scores = scores.squeeze().cpu().numpy()\n",
    "\n",
    "            heatmap = np.zeros((segments.shape[0], segments.shape[1]))\n",
    "\n",
    "            for i in range(segments.shape[0]):\n",
    "                for j in range(segments.shape[1]):\n",
    "                    heatmap[i, j] = scores[segments[i, j] - 1]\n",
    "\n",
    "            hmaps.append(heatmap)\n",
    "\n",
    "        return np.mean(np.array(hmaps), axis=0), hmaps\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=False, show_pipeline=False, show_masks=False):\n",
    "\n",
    "        if self.quiet:\n",
    "            show = False\n",
    "            show_pipeline = False\n",
    "\n",
    "        # Convert image to np array\n",
    "        np_image = img_as_float(io.imread(img_sample.path))\n",
    "\n",
    "        # Apply all methods requested\n",
    "        all_p_heatmaps = list()\n",
    "        all_heatmaps = list()\n",
    "\n",
    "        for method in self.methods:\n",
    "            p_heatmap, heatmaps = self._compute_hmap(np_image, prompt, method, self.n_segments)\n",
    "            all_p_heatmaps.append(p_heatmap)\n",
    "            all_heatmaps.append(heatmaps)\n",
    "\n",
    "        p_heatmap = np.mean(np.array(all_p_heatmaps), axis=0)\n",
    "\n",
    "        # Shut down pixels below a certain threshold\n",
    "        ths = np.quantile(p_heatmap.flatten(), self.q)\n",
    "        fp_heatmap = p_heatmap.copy()\n",
    "        fp_heatmap[p_heatmap < ths] = ths\n",
    "\n",
    "        # Downsample the heatmap by a factor d\n",
    "        dfp_heatmap = downsample_map(fp_heatmap, self.d)\n",
    "\n",
    "        # Find the best bounding box\n",
    "        pred_bbox = find_best_bbox(dfp_heatmap)\n",
    "\n",
    "        if pred_bbox is None:\n",
    "            return {\"IoU\": 0}\n",
    "\n",
    "        if self.d > 1:\n",
    "            pred_bbox = [pred_bbox[0] * self.d + self.d // 2,\n",
    "                         pred_bbox[1] * self.d + self.d // 2,\n",
    "                         pred_bbox[2] * self.d - self.d // 2,\n",
    "                         pred_bbox[3] * self.d - self.d // 2]\n",
    "\n",
    "        # Convert bbox format\n",
    "        pred_bbox = [pred_bbox[0], pred_bbox[1], pred_bbox[2] + pred_bbox[0], pred_bbox[3] + pred_bbox[1]]\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = box_iou(\n",
    "            torch.tensor([pred_bbox]),\n",
    "            torch.tensor([gt_bbox])\n",
    "        )\n",
    "\n",
    "        # Show all computed masks, if requested\n",
    "\n",
    "        if show_masks:\n",
    "            for m in range(len(self.methods)):\n",
    "                ax_cols = 4\n",
    "                ax_rows = len(self.n_segments) // ax_cols + 1\n",
    "                fig, axes = plt.subplots(ax_rows, ax_cols, figsize=(20, 5))\n",
    "                for i, ax in enumerate(axes.ravel()):\n",
    "                    ax.axis(\"off\")\n",
    "                    if i <= len(self.n_segments) - 1:\n",
    "                        ax.imshow(all_heatmaps[m])\n",
    "\n",
    "        # Show the mask processing pipeline, if requested\n",
    "        if show_pipeline:\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "            for ax in axes.ravel():\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            axes[0].imshow(np_image)\n",
    "            axes[1].imshow(p_heatmap)\n",
    "            axes[2].imshow(fp_heatmap)\n",
    "            axes[3].imshow(dfp_heatmap)\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            display_preds(sample.img, prompt, pred_bbox, sample.bbox, f\"{''.join(self.methods).upper()} + CLIP\")\n",
    "\n",
    "        return {\"IoU\": float(iou.item())}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-12T21:38:22.336140Z",
     "end_time": "2023-05-12T21:38:22.376985Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@tile Test ClipSeg on a random sample\n",
    "\n",
    "idx = np.random.randint(0, len(dataset))\n",
    "\n",
    "sample = RefCOCOgSample(**dataset[idx])\n",
    "\n",
    "clipseg = ClipSeg(methods=\"sw\", n_segments=(16, 32, 64), q=0.9)\n",
    "\n",
    "clipseg(sample, sample.sentences[0], show_pipeline=True, show_masks=False, show=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-12T21:37:11.133659Z",
     "end_time": "2023-05-12T21:38:22.329415Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Function definition to test visual grounding with a given pipeline\n",
    "\n",
    "def visual_grounding_test(vg_pipeline, dataset, track=\"IoU\"):\n",
    "    scores = list()\n",
    "\n",
    "    pbar = tqdm(dataset)\n",
    "\n",
    "    for sample in pbar:\n",
    "\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "            sc = vg_pipeline(sample, sentence, show=False)\n",
    "\n",
    "            scores.append(sc)\n",
    "\n",
    "            avg_score = np.mean([score[track] for score in scores])\n",
    "\n",
    "            pbar.set_description(f\"Average {track}: {avg_score:.3f}\")\n",
    "\n",
    "    for metric in scores[0].keys():\n",
    "        avg_metric = np.mean([score[metric] for score in scores])\n",
    "\n",
    "        print(\"Avg. {}: {:.3f}\".format(metric, avg_metric))\n",
    "\n",
    "\n",
    "clipslic = ClipSeg(methods=\"s\", n_segments=(8, 16, 32), q=0.9, quiet=True)\n",
    "\n",
    "visual_grounding_test(clipslic, test_ds, track=\"IoU\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
