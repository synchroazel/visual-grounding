{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Assignment 2023: Visual Grounding\n",
    "\n",
    "The goal of the project is to develop a deep learning framework to perform visual grounding on the RefCOCOg dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "MXShVSKTXljt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Throughout the work multiple approaches have been tested, yielding some very different results and with import differences in terms of performance. For the sake of the providing a complete overview of the work done, the notebook will go through all of the approach tested, presenting any idea that has been discussed and implemented in these months of work, discussing their pros and limitations. The approaches that have been taken into account include:\n",
    "\n",
    "- a baseline pipeline using YOLO for object proposals and CLIP for the grounding task;\n",
    "- a pipeline which involves image segmentation, CLIP embedding and bounding box proposal;\n",
    "- a pipeline using DETR for object proposals and CLIP for the grounding task;\n",
    "- a pipeline only using MDETR for the grounding task (for the sake of having a SOTA comparison)\n",
    "- a framework involving Diffusion for bounding box detection and CLIP for the grounding task.\n",
    "- a framework involving Reinforcement Learning for bounding box regression.\n",
    "\n",
    "The notebook will cover all aforementioned approaches,\n",
    "\n",
    "- (1) starting from a brief introduction, covering (i) the problem of visual grounding, (ii) a quick presentation of the dataset and (iii) CLIP\n",
    "- (2) continuing through the implementation of the different frameworks, discussing pro and cons\n",
    "- (3) and finally presenting the overall results achieved and providing some final considerations.\n",
    "\n",
    "The notebook is meant to be run standalone here on Google Colab but, may it be useful, the codebase is also available on GitHub at [synchroazel/visual-grounding](https://github.com/synchroazel/visual-grounding). Please refer to the README for further instructions on how to run it locally."
   ],
   "metadata": {
    "collapsed": false,
    "id": "GWcrIhetXljw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Brief introduction\n",
    "\n",
    "Here is a brief introduction on some of the pillars of the project, namely visual grounding, the RefCOCOg dataset and CLIP."
   ],
   "metadata": {
    "collapsed": false,
    "id": "g3fguXdBXljx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Visual Grounding\n",
    "\n",
    "Visual grounding refers to the process of linking visual information with corresponding linguistic symbols, bridging the gap between visual perception and linguistic understanding. The goal is to enable machines to comprehend and interpret visual information in a way that is similar to human understanding.\n",
    "\n",
    "Concretely, visual grounding can be approached in different ways, depending on the specific task and context. One common approach is to use techniques like object detection, image segmentation, or scene understanding to extract relevant visual features from an image or video. These features are then matched or aligned with corresponding textual descriptions or concepts using a multitude of approaches.\n",
    "\n",
    "Many of the current approaches to visual grounding can be identified either as a **one-stage** or **two-stage**.\n",
    "\n",
    "- Two-stage methods formulate visual grounding as a matching problem between language and region. The visual region proposals are extracted by a pre-trained detector in the first stage, which are matched with the given expression in the second stage. However, the performance of these methods is highly dependent on the detector in the first stage. Besides, matching must be performed for every region proposals, which drag a great extent on the speed of the network.\n",
    "\n",
    "- One-stage methods overcome the reliance on detectors and speed up the inference process by grounding the object in an image by a sentence query directly.\n",
    "\n",
    "Today, SOTA approaches to visual grounding are relying more and more on one-stage methods, especially using transformers-based architectures. In this project the main focus will be on how to include and repurpose CLIP for the task, yet an implementation of MDETR will still be shown and  rbiefly discussed."
   ],
   "metadata": {
    "collapsed": false,
    "id": "EoSxoWXxXljx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 RefCOCOg Dataset\n",
    "\n",
    "The dataset used for this task is RefCOCOg, consisting of 49822 images with one or more referring expression/s each, for a total of 95010 sentences.\n",
    "\n",
    "Differently from RefCOCO & RefCOCO+, which were obtained through the 2-players Refer-It game, RefCOCOg was collected on Amazon Mechanical Turk in a non-interactive setting. One set of workers were asked to write natural language referring expressions for objects in MSCOCO images then another set of workers were asked to click on the indicated object given a referring expression. If the click overlapped with the correct object then the referring expression was considered valid and added to the dataset. If not, another referring expression was collected for the object.\n",
    "\n",
    "...\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "9X_xiFMzXljy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Contrastive Language-Image Pre-Training (CLIP)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Sh2JCiBQXljz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Preliminary steps\n",
    "\n",
    "Here are some preliminary steps before we start discussing each approach in more detail."
   ],
   "metadata": {
    "collapsed": false,
    "id": "PdyYCr-4Xljz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Install all necessary pacakges\n",
    "\n",
    "!pip install transformers\n",
    "!pip install ultralytics\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install torchmultimodal-nightly\n"
   ],
   "metadata": {
    "id": "C6dVFsShYnRH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-04T12:29:13.473538Z",
     "end_time": "2023-06-04T12:29:13.581217Z"
    },
    "id": "IsWHw1vyXlj0"
   },
   "outputs": [],
   "source": [
    "#@title Import all necessary modules\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import clip\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import slic, watershed\n",
    "from skimage.util import img_as_float\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torchmultimodal.models.mdetr.model import mdetr_for_phrase_grounding\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.ops.boxes import box_convert\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, DetrForObjectDetection\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers.utils import logging\n",
    "from ultralytics import YOLO\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] No GPU found, using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#@title Set the best device for to the machine running the notebook\n",
    "\n",
    "def get_best_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device_ = torch.device(\"cuda\")  # for CUDA GPU\n",
    "        print(\"[INFO] Using cuda.\")\n",
    "    elif torch.has_mps:\n",
    "        device_ = torch.device(\"mps\")  # for Apple Silicon GPU\n",
    "        print(\"[INFO] Using MPS.\")\n",
    "    else:\n",
    "        device_ = torch.device(\"cpu\")\n",
    "        print(\"[INFO] No GPU found, using CPU instead.\")\n",
    "\n",
    "    return device_\n",
    "\n",
    "\n",
    "device = get_best_device()\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-04T12:17:24.647965Z",
     "end_time": "2023-06-04T12:17:24.693540Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xir1IytjXlj1",
    "outputId": "89e55d37-c40c-4341-e3b0-191d44204757"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is also important to define our dataset and sample objects, which are going to be used throughout the notebook.\n",
    "\n",
    "- The `RefCOCOgSample` object define a single sample from the RefCOCOg dataset, containing all the information - as attributes - that may be needed for visual grounding tasks.\n",
    "\n",
    "- The `RefCOCOg` object is a wrapper around the RefCOCOg dataset, providing some useful methods to access the dataset and its samples. Most notably, it implements (1) a `__getitem__` method to access a single sample and (2) a `__len__` method to get the number of samples in the dataset. Also notice that the `__getitem__` method returns a dictionary containing the information that can be used to populate a `RefCOCOgSample` object (simply with `RefCOCOgSample(**output)`). The `__init__` method also takes care of loading the correct split we might need, accepting either `train`, `val` or `test` as arguments and using the annotations from `refs(umd).p` to select the samples of each."
   ],
   "metadata": {
    "collapsed": false,
    "id": "PGJQT-vOXlj2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for a sample from the RefCOCOg dataset\n",
    "\n",
    "class RefCOCOgSample:\n",
    "    \"\"\"\n",
    "    An annotated image from RefCOCOg dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 img: Image.Image,\n",
    "                 shape: tuple[int, int],\n",
    "                 path: str,\n",
    "                 img_id: str,\n",
    "                 split: str,\n",
    "                 category: str,\n",
    "                 category_id: int,\n",
    "                 sentences: list[str],\n",
    "                 bbox: list[float],\n",
    "                 segmentation: list[float]):\n",
    "        self.img = img\n",
    "        self.shape = shape\n",
    "        self.path = path\n",
    "        self.id = img_id\n",
    "        self.split = split\n",
    "        self.category = category\n",
    "        self.category_id = category_id\n",
    "        self.sentences = sentences\n",
    "        self.bbox = bbox\n",
    "        self.segmentation = segmentation\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(vars(self))\n"
   ],
   "metadata": {
    "id": "GWTWVga_Xlj2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the RefCOCOg dataset\n",
    "\n",
    "class RefCOCOg(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object for RefCOCOg dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds_path: str, split=None, transform=None):\n",
    "        super(RefCOCOg, self).__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.ds_path = ds_path\n",
    "\n",
    "        with open(f'{ds_path}/annotations/refs(umd).p', 'rb') as f:\n",
    "            self.refs = pickle.load(f)\n",
    "\n",
    "        with open(f\"{ds_path}/annotations/instances.json\", \"r\") as f:\n",
    "            self.instances = json.load(f)\n",
    "\n",
    "        self.categories = {\n",
    "            item[\"id\"]: {\n",
    "                \"supercategory\": item[\"supercategory\"],\n",
    "                \"category\": item[\"name\"]\n",
    "            }\n",
    "            for item in self.instances['categories']\n",
    "        }\n",
    "\n",
    "        self.instances = {inst['id']: inst for inst in self.instances['annotations']}\n",
    "\n",
    "        if split == 'train':\n",
    "            self.refs = [ref for ref in self.refs if ref['split'] == 'train']\n",
    "        elif split == 'val':\n",
    "            self.refs = [ref for ref in self.refs if ref['split'] == 'val']\n",
    "        elif split == 'test':\n",
    "            self.refs = [ref for ref in self.refs if ref['split'] == 'test']\n",
    "\n",
    "        self.size = len(self.refs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        refs_data = self.refs[idx]\n",
    "\n",
    "        ann_data = self.instances[refs_data['ann_id']]\n",
    "\n",
    "        image_path = os.path.join(\n",
    "            self.ds_path,\n",
    "            \"images\",\n",
    "            re.sub(r\"_[0-9]+\\.jpg\", \".jpg\", refs_data[\"file_name\"])\n",
    "        )\n",
    "\n",
    "        pil_img = Image.open(image_path)\n",
    "\n",
    "        bbox = torch.tensor(ann_data[\"bbox\"])\n",
    "        bbox = box_convert(bbox, \"xywh\", \"xyxy\").numpy()\n",
    "\n",
    "        sample = {\n",
    "            \"img\": pil_img,\n",
    "            \"shape\": transforms.ToTensor()(pil_img).shape,\n",
    "            \"path\": image_path,\n",
    "            \"img_id\": refs_data[\"image_id\"],\n",
    "            \"split\": refs_data[\"split\"],\n",
    "            \"category\": self.categories[refs_data[\"category_id\"]][\"category\"],\n",
    "            \"category_id\": refs_data[\"category_id\"],\n",
    "            \"sentences\": [sentence[\"raw\"].lower() for sentence in refs_data[\"sentences\"]],\n",
    "            \"bbox\": bbox,\n",
    "            \"segmentation\": ann_data[\"segmentation\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample[\"img\"], dtype=torch.float32)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size  # return the number of annotated images available using `refs(umd).p`\n"
   ],
   "metadata": {
    "id": "ybHOOsmmXlj3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that taken care of, we can now import the dataset and its split."
   ],
   "metadata": {
    "collapsed": false,
    "id": "j_kL86NVXlj3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Import the RefCOCOg dataset and create train/validation/test splits\n",
    "\n",
    "data_path = \"dataset/refcocog\"\n",
    "\n",
    "dataset = RefCOCOg(ds_path=data_path)\n",
    "\n",
    "train_ds = RefCOCOg(ds_path=data_path, split='train')\n",
    "val_ds = RefCOCOg(ds_path=data_path, split='val')\n",
    "test_ds = RefCOCOg(ds_path=data_path, split='test')\n",
    "\n",
    "print(f\"[INFO] Dataset Size: {len(dataset)}\")\n",
    "print(f\"[INFO] train split:  {len(train_ds)}\")\n",
    "print(f\"[INFO] val split:    {len(val_ds)}\")\n",
    "print(f\"[INFO] test split:   {len(test_ds)}\")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-04T12:17:24.693787Z",
     "end_time": "2023-06-04T12:17:35.811084Z"
    },
    "id": "Aj0spHKpXlj4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. The pipelines\n",
    "\n",
    "In the following section we are going to present the different visual grounding pipelines that have been implemented and tested."
   ],
   "metadata": {
    "collapsed": false,
    "id": "QgUeYl1wXlj4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**About the metrics**\n",
    "\n",
    "Before diving into each approach, it is worth mentioning the metrics that have been used to evaluate the performance of each pipeline. The metrics used aim to evaluate the performance of the framework in terms of\n",
    "\n",
    "(1) localization accuracy\n",
    "(2) grounding accuracy\n",
    "(3) semantic similarity\n",
    "\n",
    "Localization accuracy refers to the ability of the model to localize an object in the image, and it is measured using **Intersection over Union (IoU)**, namely the ratio between the area of overlap between the predicted bounding box and the ground-truth bounding box and the area of union between the two.\n",
    "\n",
    "Semantic similarity refers to the similarity between the predicted bounding boxes and the ground-truth descriptions, and it is measured using distance metrics such as cosine similarity, **euclidean distance** or **dot product** between the CLIP embeddings of the predicted bounding boxes and the ground-truth textual description.\n",
    "\n",
    "Grounding accuracy refers to the ability of the model to ground the localized object to a language description, and it is measured using **recall**, namely the ratio between the number of correctly grounded objects and the total number of objects. Practically\n",
    "\n",
    "- we get a CLIP encoding of each available category in a dummy `f\"a picture of a {object}\"` sentence\n",
    "- we get a CLIP encoding of the bounding boxed image proposed by the pipeline\n",
    "- we compare those and get the category that is most similar\n",
    "- if the category is the same as the one in the ground truth, we have a correct grounding attempt\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "tUY-rJaHXlj5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**About the implementation**\n",
    "\n",
    "To simplify our codebase, and to provide extra readability, we used a common superclass `VisualGroundingPipeline` for all the visual grounding frameworks approached. This superclass provides a common interface for all the pipelines, such as\n",
    "\n",
    "- (1) initialization of common attributes\n",
    "- (2) a pair of methods to encode text and images using CLIP after taking care of tokenization/preprocess\n",
    "- (3) a method to compute IoU\n",
    "- (4) a method to compute the grounding accuracy and finally a method used to embed, on pipeline instantiation, all the available categories (useful, as mentioned, for the visual grounding accuracy computation).\n",
    "\n",
    "All pipelines inherit from this superclass, and implement one or more custom methods, most notably one of them being a `__call__` method which contains the core logic and returns the metrics of interest.\n",
    "\n",
    "At subclass level each pipeline also implement a set of common attributes to be used in the `__call__` method for displaying and testing purposes, namely\n",
    "\n",
    "- `show=True` will display the image with the predicted bounding box\n",
    "- `timeit=True` will print the time taken to run the pipeline (it is recommended to use it without any visualization)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6vmeO5wmXlj5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define utilities function used throughout the notebook\n",
    "\n",
    "def IoU(true_bbox, predicted_bbox):\n",
    "    # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(true_bbox[0], predicted_bbox[0])\n",
    "    yA = max(true_bbox[1], predicted_bbox[1])\n",
    "    xB = min(true_bbox[2], predicted_bbox[2])\n",
    "    yB = min(true_bbox[3], predicted_bbox[3])\n",
    "\n",
    "    # Compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    true_bboxArea = (true_bbox[2] - true_bbox[0] + 1) * (true_bbox[3] - true_bbox[1] + 1)\n",
    "    predicted_bboxArea = (predicted_bbox[2] - predicted_bbox[0] + 1) * (predicted_bbox[3] - predicted_bbox[1] + 1)\n",
    "\n",
    "    # Compute the intersection over union by taking the intersection area\n",
    "    # and dividing it by the sum of prediction + ground-truth areas - the intersection area\n",
    "    iou = interArea / float(true_bboxArea + predicted_bboxArea - interArea)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def get_data(dataset):\n",
    "    texts, images = list(), list()\n",
    "\n",
    "    for sample in tqdm(dataset, desc=\"[INFO] Loading images and captions\"):\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "            images.append(sample.path)\n",
    "            texts.append(sentence)\n",
    "\n",
    "    return images, texts\n",
    "\n",
    "\n",
    "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
    "    # normalise the image and the text\n",
    "    images_z /= images_z.norm(dim=-1, keepdim=True)\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # evaluate the cosine similarity between the sets of features\n",
    "    similarity = (texts_z @ images_z.T)\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def display_preds(img, prompt, pred_bbox, gt_bbox, model_name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    pred_rect = plt.Rectangle(\n",
    "        (pred_bbox[0], pred_bbox[1]), pred_bbox[2] - pred_bbox[0], pred_bbox[3] - pred_bbox[1],\n",
    "        linewidth=1.5, edgecolor=(0, 1, 0), facecolor='none'\n",
    "    )\n",
    "\n",
    "    gt_rect = plt.Rectangle(\n",
    "        (gt_bbox[0], gt_bbox[1]), gt_bbox[2] - gt_bbox[0], gt_bbox[3] - gt_bbox[1],\n",
    "        linewidth=1.5, edgecolor=(1, 0, 0), facecolor='none'\n",
    "    )\n",
    "\n",
    "    ax.add_patch(pred_rect)\n",
    "    ax.text(pred_bbox[0], pred_bbox[1], \"predicted\", color=(1, 1, 1),\n",
    "            bbox={\"facecolor\": (0, 1, 0), \"edgecolor\": (0, 1, 0), \"pad\": 2})\n",
    "\n",
    "    ax.add_patch(gt_rect)\n",
    "    ax.text(gt_bbox[0], gt_bbox[3], \"true\", color=(1, 1, 1),\n",
    "            bbox={\"facecolor\": (1, 0, 0), \"edgecolor\": (1, 0, 0), \"pad\": 2})\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    plt.title(f\"\\\"{prompt.capitalize()}\\\"\\n\")\n",
    "    plt.text(0.5, -0.075, f\"using {model_name}\", size=10, ha=\"center\", transform=ax.transAxes)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "PjYlMvbhXlj6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the VisualGroundingPipeline superclass\n",
    "\n",
    "class VisualGroundingPipeline:\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 clip_ver=\"RN50\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "        self.categories = copy.deepcopy(categories)\n",
    "        self.clip_ver = clip_ver\n",
    "        self.clip_model, self.clip_prep = clip.load(clip_ver, device=\"cpu\")\n",
    "        self.device = device\n",
    "        self.quiet = quiet\n",
    "\n",
    "        # model is loaded to cpu first, and eventually moved to gpu\n",
    "        # (trick Mac M1 to use f16 tensors)\n",
    "        if self.device != \"cpu\":\n",
    "            self.clip_model = self.clip_model.to(self.device)\n",
    "\n",
    "        self._embed_categories()\n",
    "\n",
    "    def _encode_text(self, text):\n",
    "        text_ = clip.tokenize(text).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self.clip_model.encode_text(text_)\n",
    "\n",
    "    def _encode_img(self, image):\n",
    "        image_ = self.clip_prep(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self.clip_model.encode_image(image_)\n",
    "\n",
    "    @staticmethod\n",
    "    def _IoU(pred_bbox, gt_bbox):\n",
    "        iou = box_iou(\n",
    "            torch.tensor(pred_bbox).unsqueeze(0),\n",
    "            torch.tensor(gt_bbox).unsqueeze(0)\n",
    "        ).item()\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def _grounding_accuracy(self, img_sample, pred_image_enc):\n",
    "        all_c_sims = dict()\n",
    "\n",
    "        for category_id in self.categories.keys():\n",
    "            cur_categ = self.categories[category_id]['category']\n",
    "            cur_categ_enc = self.categories[category_id]['encoding'].float()\n",
    "\n",
    "            all_c_sims[cur_categ] = cosine_similarity(pred_image_enc, cur_categ_enc)\n",
    "\n",
    "        pred_category = max(all_c_sims, key=all_c_sims.get)\n",
    "\n",
    "        # if not self.quiet:\n",
    "        #     print(f\"[INFO] true: {img_sample.category} | predicted: {pred_category}\")\n",
    "\n",
    "        return 1 if pred_category == img_sample.category else 0\n",
    "\n",
    "    def _embed_categories(self):\n",
    "        for category_id in self.categories.keys():\n",
    "            cur_category = self.categories[category_id]['category']\n",
    "            with torch.no_grad():\n",
    "                cur_category_enc = self._encode_text(f\"a photo of {cur_category}\")\n",
    "            self.categories[category_id].update({\"encoding\": cur_category_enc})\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return None"
   ],
   "metadata": {
    "id": "f2_zo-lpXlj6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, before skipping ahead, we pick a random a sample image from the dataset to use as a quick test for the following sections."
   ],
   "metadata": {
    "collapsed": false,
    "id": "63yPVMU1Xlj7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Pick a random sample from the dataset\n",
    "\n",
    "idx = np.random.randint(0, len(dataset))\n",
    "sample = RefCOCOgSample(**dataset[idx])\n",
    "\n",
    "plt.imshow(sample.img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Sample #{idx}\", loc=\"left\")\n",
    "plt.show()\n",
    "\n",
    "for i, sentence in enumerate(sample.sentences):\n",
    "    print(f\"[INFO] Sentence #{i}: {sentence}\")\n"
   ],
   "metadata": {
    "id": "rkJ5T0i1Xlj7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 YOLO + CLIP\n",
    "\n",
    "The first approach that has been tested is a baseline pipeline using **YOLO** for object proposals and CLIP for the grounding task. The idea is to use YOLO to extract the bounding boxes of the objects in an image and then use CLIP to find the best match between the bounding boxes and the sentence query. This latter part is simply done by computing CLIP embeddings of the cropped bounding boxes and the sentence query and then computing the cosine similarity between the two embeddings. The bounding box with the highest similarity score is then selected as the best match.\n",
    "\n",
    "**YOLO**, which funnily stands for \"You Only Look Once,\" is a popular object detection model in computer vision. It revolutionized real-time object detection by proposing a unified framework that simultaneously predicts object bounding boxes and class probabilities in a single pass. YOLO divides the input image into a grid and applies convolutional neural networks to each grid cell to predict bounding boxes and class probabilities. This approach allows YOLO to achieve impressive detection speeds while maintaining competitive accuracy.\n",
    "\n",
    "This approach has been tested both using different version of YOLO (YOLOv8x and YOLOv5su - which recently replaced YOLOv5s) and different visual backbones for CLIP (ResNet50, ResNet101 and ViT-L/14). The results are shown in the following table."
   ],
   "metadata": {
    "collapsed": false,
    "id": "5tFjf93uXlj7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the YOLO+Clip pipeline\n",
    "\n",
    "class YoloClip(VisualGroundingPipeline):\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 yolo_ver=\"yolov8x\",\n",
    "                 clip_ver=\"RN50\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        self.yolo_ver = yolo_ver\n",
    "        self.yolo_model = YOLO(self.yolo_ver + \".pt\")\n",
    "\n",
    "        valid_yolo_versions = [\"yolov8x\", \"yolov5su\"]\n",
    "        if yolo_ver not in valid_yolo_versions:\n",
    "            raise ValueError(f\"Invalid YOLO version '{yolo_ver}'. Must be one of {valid_yolo_versions}.\")\n",
    "\n",
    "        print(\"[INFO] Initializing YoloClip pipeline\")\n",
    "        print(f\"[INFO] YOLO version: {yolo_ver}\")\n",
    "        print(\"\")\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=False, show_yolo=False, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        # Get sample image\n",
    "        img = img_sample.img\n",
    "\n",
    "        # Use YOLO to propose relevant objects\n",
    "        yolo_results_ = self.yolo_model(img_sample.path, verbose=False)[0]\n",
    "        yolo_results = yolo_results_.boxes.xyxy\n",
    "        if not self.quiet:\n",
    "            print(f\"[INFO] YOLO found {yolo_results.shape[0]} objects\")\n",
    "        if yolo_results.shape[0] == 0:\n",
    "            print(f\"[WARN] YOLO ({self.yolo_ver}) couldn't find any object in {img_sample.path}!\")\n",
    "            return {\"IoU\": 0, \"cosine\": np.nan, \"euclidean\": np.nan, \"dotproduct\": np.nan, \"grounding\": np.nan}\n",
    "\n",
    "        # Use CLIP to encode each relevant object image\n",
    "        images_encs = list()\n",
    "        for i in range(yolo_results.shape[0]):\n",
    "            bbox = yolo_results[i, 0:4].cpu().numpy()\n",
    "            sub_img = img.crop(bbox)\n",
    "            with torch.no_grad():\n",
    "                sub_img_enc = self._encode_img(sub_img)\n",
    "            images_encs.append(sub_img_enc)\n",
    "        images_encs = torch.cat(images_encs, dim=0)\n",
    "\n",
    "        # Use CLIP to encode the text prompt\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        # Compute the best bbox according to cosine similarity\n",
    "        c_sims = cosine_similarity(prompt_enc, images_encs).squeeze()\n",
    "        best_idx = int(c_sims.argmax())\n",
    "\n",
    "        # Get best bbox\n",
    "        pred_bbox = yolo_results[best_idx, 0:4].tolist()\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T  # dot product\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)  # cosine similarity\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()  # euclidean distance\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show objects found by YOLO, if requested\n",
    "        if show_yolo:\n",
    "            plt.imshow(yolo_results_.plot())\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"YOLO findings\")\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            display_preds(img, prompt, pred_bbox, gt_bbox, model_name=f\"{self.yolo_ver} + CLIP({self.clip_ver})\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "id": "aZnTt_CPXlj8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that said, we can instantiate two object for the YOLO+CLIP pipeline, using different YOLO versions, and quickly see them running."
   ],
   "metadata": {
    "collapsed": false,
    "id": "vFC1cPUXXlj8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate YOLO+CLIP pipelines\n",
    "\n",
    "# YOLOv5 + CLIP\n",
    "yolo5clip = YoloClip(dataset.categories, yolo_ver=\"yolov5su\", quiet=True, device=device)\n",
    "\n",
    "# YOLOv8 + CLIP\n",
    "yolo8clip = YoloClip(dataset.categories, yolo_ver=\"yolov8x\", quiet=True, device=device)\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-04T12:17:35.812635Z",
     "end_time": "2023-06-04T12:18:13.701099Z"
    },
    "id": "qJ6rDBCMXlj8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the YOLOv5 + CLIP pipeline\n",
    "\n",
    "#@markdown Please flag `show` to show prediction, or `timeit` to time the process.<br>\n",
    "#@markdown Also, `show_yolo` will display YOLO detections.\n",
    "show = True #@param {type:\"boolean\"}\n",
    "timeit = False #@param {type:\"boolean\"}\n",
    "show_yolo = True #@param {type:\"boolean\"}\n",
    "\n",
    "yolo5clip(sample, sample.sentences[0], show_yolo=show_yolo, show=show, timeit=timeit)\n"
   ],
   "metadata": {
    "id": "BbXkMTFcXlj8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the YOLOv8 + CLIP pipeline\n",
    "\n",
    "yolo8clip(sample, sample.sentences[0], show_yolo=True, show=True, timeit=False)\n"
   ],
   "metadata": {
    "id": "an-tuateXlj9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Segmentation + CLIP\n",
    "\n",
    "This approach is fundamentally based on the idea of segmenting the sample image into a set of regions and interpreting the similarity of each section to the prompt as a heatmap. A heatmap in this context is obtained as follows:\n",
    "\n",
    "1. The sample image is segmented using a segmentation algorithm (e.g. SLIC, Watershed)\n",
    "2. Each region is encoded using CLIP\n",
    "3. The similarity between the prompt and each region is computed\n",
    "4. Each \"pixel\" inside each region is assigned the similarity score of that region, producing a score heatmap\n",
    "\n",
    "That being said, simply computing one single heatmap may not be enough to capture both larger and fine-grained image-text similarites. For such reasons, we compute different heatmaps using a different number of segments each time, and finally pooling all the heatmap together using the mean score of each pixel. This is not all though. At this point:\n",
    "\n",
    "1. All pixels below a certain threshold are turned off (set to 0)\n",
    "2. The heatmap is downsampled with a certain factor for lighter computations\n",
    "3. The heatmap is normalized to the range [-1, 1]\n",
    "4. The best bounding box is extracted from the heatmap, by considering all possible bounding boxes and selecting the one with the highest pixel sum\n",
    "\n",
    "Note that:\n",
    "- the threshold used to filter out pixels is taken as the value of a certain quantile `q` of the pixel values distribution\n",
    "- the bbox search algorithm searches over all possible boxes, and that is why the heatmap is downsampled first\n",
    "- the normalization also serves to give turned-off pixels negative values and discourage the algorithm from selecting areas containing many\n",
    "\n",
    "Although being pretty flexible in terms of experimentation with hyperparameters, this model suffers from a few limitations, one of them being the slow inference time. This could be definitely improved in the future by defining another bounding box search algorithm, such as [...].\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "AjPamg_eXlj9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Some implementative details***\n",
    "\n",
    "The class implements a `__compute_hmap` method, which is responsible for computing the heatmap by passing a method and a number of segments to use it with. The method is used by `__call__` which performs the other steps described above and returns the metrics. The `_find_best_bbox` method is responsible for bbox searching, as decribed.\n",
    "\n",
    "When calling the pipeline, the user can specify two additional parameters:\n",
    "\n",
    "- `show_process=True` will show the resulting heatmap alongside its filtered and downsampled versions\n",
    "- `show_masks=True` will show all *N* masks before pooling, with *N* being the number of segments used\n",
    "\n",
    "Also, note that the hyperparameters specified in the class instantiation below are those which, experimentally, gave the best results."
   ],
   "metadata": {
    "collapsed": false,
    "id": "fFl6EEW3Xlj-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the Segmentation + CLIP pipeline\n",
    "\n",
    "class ClipSeg(VisualGroundingPipeline):\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 method,\n",
    "                 n_segments,\n",
    "                 clip_ver=\"ViT-L/14\",\n",
    "                 q=0.95,\n",
    "                 d=16,\n",
    "                 device=\"cpu\",\n",
    "                 quiet=False):\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        self.method = method\n",
    "        self.n_segments = n_segments\n",
    "        self.q = q\n",
    "        self.d = d\n",
    "\n",
    "        valid_methods = [\"s\", \"w\"]\n",
    "        if self.method not in valid_methods:\n",
    "            raise ValueError(f\"Method `{method}` not supported. Supported methods are: {valid_methods}.\")\n",
    "\n",
    "        print(\"[INFO] Initializing ClipSeg pipeline\")\n",
    "        print(f\"[INFO] Segmentation method: {method}\")\n",
    "        print(f\"[INFO] Number of segments: {n_segments}\")\n",
    "        print(f\"[INFO] Threshold q.tile for filtering: {q}\")\n",
    "        print(f\"[INFO] Downsampling factor: {d}\")\n",
    "        print(\"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _downsample_map(self, hmap, factor):\n",
    "        # number of blocks in each dimension\n",
    "        blocks_h = hmap.shape[0] // factor\n",
    "        blocks_w = hmap.shape[1] // factor\n",
    "\n",
    "        # reshape the original matrix into blocks\n",
    "        blocks = hmap[:blocks_h * factor, :blocks_w * factor].reshape(blocks_h, factor, blocks_w, factor)\n",
    "\n",
    "        # calculate the average of each block\n",
    "        averages = blocks.mean(axis=(1, 3))\n",
    "\n",
    "        return averages\n",
    "\n",
    "    def _compute_hmap(self, img_sample, np_image, prompt, method, masks):\n",
    "\n",
    "        # Make sure np_image is an image with shape (h, w, 3)\n",
    "        if len(np_image.shape) > 3 or (len(np_image.shape) == 3 and np_image.shape[-1] != 3):\n",
    "            np_image = np_image[:, :, 0]\n",
    "\n",
    "        if len(np_image.shape) == 2:\n",
    "            np_image = np.stack((np_image,) * 3, axis=-1)\n",
    "\n",
    "        hmaps = list()\n",
    "\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        for i, n in enumerate(masks):\n",
    "\n",
    "            # Compute regions according to chosen method\n",
    "            segments = None\n",
    "            if method == \"s\":\n",
    "                # SLIC segmentation algorithm ()\n",
    "                segments = slic(np_image, n_segments=n, compactness=10, sigma=1)\n",
    "            elif method == \"w\":\n",
    "                # Watershed segmentation algorithm ()\n",
    "                segments = watershed(sobel(rgb2gray(np_image)), markers=n, compactness=0.001)\n",
    "\n",
    "            if segments is None:\n",
    "                raise Exception(\"Segments are None. Is method different from 's' or 'w'? \")\n",
    "\n",
    "            regions = regionprops(segments)\n",
    "\n",
    "            if len(regions) == 1:\n",
    "                # If the algo returned only 1 region, skip this iteration\n",
    "                # (may happen, with low-segments masks)\n",
    "                continue\n",
    "\n",
    "            # Compute CLIP encodings for each region\n",
    "\n",
    "            images_encs = list()\n",
    "\n",
    "            regions = tqdm(regions, desc=f\"[INFO] Computing CLIP masks\", leave=False) if not self.quiet else regions\n",
    "\n",
    "            for region in regions:\n",
    "                rect = region.bbox\n",
    "                rect = (rect[1], rect[0], rect[3], rect[2])\n",
    "\n",
    "                sub_image = img_sample.img.crop(rect)\n",
    "                image_enc = self._encode_img(sub_image)\n",
    "                images_encs.append(image_enc)\n",
    "\n",
    "            # Assign a score to each region according to prompt similarity (creating a heatmap)\n",
    "\n",
    "            images_encs = torch.cat(images_encs, dim=0)\n",
    "            scores = prompt_enc @ images_encs.T\n",
    "            scores = scores.squeeze().cpu().numpy()\n",
    "            heatmap = np.zeros((segments.shape[0], segments.shape[1]))\n",
    "\n",
    "            for i in range(segments.shape[0]):\n",
    "                for j in range(segments.shape[1]):\n",
    "                    heatmap[i, j] = scores[segments[i, j] - 1]\n",
    "\n",
    "            hmaps.append(heatmap)\n",
    "\n",
    "        # Finally, return the pooled heatmap and the list of all heatmaps computed\n",
    "\n",
    "        pmap = np.mean(np.array(hmaps), axis=0)\n",
    "\n",
    "        return pmap, hmaps\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_best_bbox(self, heatmap, lower_bound=-1.0, upper_bound=1.0):\n",
    "        # Rescale the heatmap\n",
    "        heatmap = MinMaxScaler(feature_range=(lower_bound, upper_bound)).fit_transform(heatmap)\n",
    "\n",
    "        # Initialize the best score and best box\n",
    "        best_score = float('-inf')\n",
    "        best_box = None\n",
    "\n",
    "        # Loop over all possible box sizes and positions\n",
    "        for w in range(1, heatmap.shape[1] + 1):\n",
    "            for h in range(1, heatmap.shape[0] + 1):\n",
    "                for i in range(heatmap.shape[1] - w + 1):\n",
    "                    for j in range(heatmap.shape[0] - h + 1):\n",
    "\n",
    "                        # Get current sub-region\n",
    "                        candidate = heatmap[j:j + h, i:i + w]\n",
    "\n",
    "                        # Compute the score for this box\n",
    "                        score = candidate.sum()\n",
    "\n",
    "                        # Update the best score and best box if necessary\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_box = (i, j, w, h)\n",
    "\n",
    "        best_box = [best_box[0], best_box[1], best_box[2] + best_box[0], best_box[3] + best_box[1]]\n",
    "\n",
    "        return best_box\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=False, show_process=False, show_masks=False, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        \"\"\" Pipeline core \"\"\"\n",
    "\n",
    "        # Get sample image\n",
    "        img = img_sample.img\n",
    "\n",
    "        # Convert image to np array\n",
    "        np_image = img_as_float(io.imread(img_sample.path))\n",
    "\n",
    "        # Compute a heatmap of CLIP scores\n",
    "        p_heatmap, heatmaps = self._compute_hmap(img_sample, np_image, prompt, self.method, self.n_segments)\n",
    "\n",
    "        # Shut down pixels below a certain threshold\n",
    "        ths = np.quantile(p_heatmap.flatten(), self.q)\n",
    "        fp_heatmap = p_heatmap.copy()\n",
    "        fp_heatmap[p_heatmap < ths] = ths\n",
    "\n",
    "        # Downsample the heatmap by a factor d\n",
    "        dfp_heatmap = self._downsample_map(fp_heatmap, self.d)\n",
    "\n",
    "        # Find the best bounding box\n",
    "        pred_bbox = self._find_best_bbox(dfp_heatmap, lower_bound=-0.75)\n",
    "\n",
    "        if pred_bbox is None:\n",
    "            return {\"IoU\": 0, \"cosine\": np.nan, \"euclidean\": np.nan, \"dotproduct\": np.nan, \"grounding\": np.nan}\n",
    "\n",
    "        if self.d > 1:\n",
    "            pred_bbox = [pred_bbox[0] * self.d + self.d // 2,\n",
    "                         pred_bbox[1] * self.d + self.d // 2,\n",
    "                         pred_bbox[2] * self.d - self.d // 2,\n",
    "                         pred_bbox[3] * self.d - self.d // 2]\n",
    "\n",
    "        # Use CLIP to encode the text prompt\n",
    "        prompt_enc = self._encode_text(prompt).float()\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T  # dot product\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)  # cosine similarity\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()  # euclidean distance\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show all masks, if requested\n",
    "        if show_masks:\n",
    "            fig, axes = plt.subplots(1, len(heatmaps), figsize=(20, 5))\n",
    "            for i, heatmap in enumerate(heatmaps):\n",
    "\n",
    "                for ax in axes.ravel():\n",
    "                    ax.axis(\"off\")\n",
    "\n",
    "                axes[i].imshow(np_image, alpha=0.25)\n",
    "                axes[i].imshow(heatmap, alpha=0.75)\n",
    "                axes[i].set_title(f\"#{i + 1}\")\n",
    "\n",
    "        # Show the mask processing pipeline, if requested\n",
    "        if show_process:\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "            for ax in axes.ravel():\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            axes[0].imshow(np_image)\n",
    "            axes[0].set_title(\"original image\")\n",
    "\n",
    "            axes[1].imshow(np_image, alpha=0.25)\n",
    "            axes[1].imshow(p_heatmap, alpha=0.75)\n",
    "            axes[1].set_title(\"pooled heatmap\")\n",
    "\n",
    "            axes[2].imshow(np_image, alpha=0.25)\n",
    "            axes[2].imshow(fp_heatmap, alpha=0.75)\n",
    "            axes[2].set_title(\"filtered heatmap\")\n",
    "\n",
    "            axes[3].imshow(np_image, alpha=0.25)\n",
    "            w, h = np_image.shape[1], np_image.shape[0]\n",
    "            dfp_heatmap_ = cv2.resize(dfp_heatmap, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            axes[3].imshow(dfp_heatmap_, alpha=0.75)\n",
    "            axes[3].set_title(\"dsampled heatmap\")\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            methods = {\"w\": \"Watershed\", \"s\": \"SLIC\"}\n",
    "            display_preds(img_sample.img, prompt, pred_bbox, img_sample.bbox,\n",
    "                          f\"{methods[self.method]} + CLIP ({self.clip_ver})\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "id": "WJ9npwhjXlj-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that said, we can instantiate two object for the Segmentation+CLIP pipeline, which we will conveniently call after the segmentation method used."
   ],
   "metadata": {
    "collapsed": false,
    "id": "DeuQLsewXlj-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate the segmentation + CLIP pipelines\n",
    "\n",
    "# Watershed seg. + CLIP | pooling maps with 4, 8, 16, 32 segments | filtering below 0.75 q.tile\n",
    "wshedclip = ClipSeg(dataset.categories, method=\"w\", n_segments=(4, 8, 16, 32), q=0.75, quiet=False, device=device)\n",
    "\n",
    "# SLIC seg. + CLIP | pooling maps with 4, 8, 16, 32 segments | filtering below 0.75 q.tile\n",
    "slicnclip = ClipSeg(dataset.categories, method=\"s\", n_segments=(4, 8, 16, 32), q=0.75, quiet=False, device=device)\n"
   ],
   "metadata": {
    "id": "0r6EYsZhXlj-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the Watershed + CLIP pipeline\n",
    "\n",
    "#@markdown Please flag `show` to show prediction, or `timeit` to time the process.<br>\n",
    "#@markdown `show_process` will display the processing steps of the heatmap.<br>\n",
    "#@markdown `show_masks` will display the computed CLIP heatmaps.\n",
    "show = True #@param {type:\"boolean\"}\n",
    "timeit = False #@param {type:\"boolean\"}\n",
    "show_process = True #@param {type:\"boolean\"}\n",
    "show_masks = True #@param {type:\"boolean\"}\n",
    "\n",
    "\n",
    "wshedclip(sample, sample.sentences[0], show_process=True, show_masks=True, show=True, timeit=False)\n"
   ],
   "metadata": {
    "id": "hkZQ7lpKXlj_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the SLIC + CLIP pipeline\n",
    "\n",
    "#@markdown Please flag `show` to show prediction, or `timeit` to time the process.<br>\n",
    "#@markdown `show_process` will display the processing steps of the heatmap.<br>\n",
    "#@markdown `show_masks` will display the computed CLIP heatmaps.\n",
    "show = True #@param {type:\"boolean\"}\n",
    "timeit = False #@param {type:\"boolean\"}\n",
    "show_process = True #@param {type:\"boolean\"}\n",
    "show_masks = True #@param {type:\"boolean\"}\n",
    "\n",
    "slicnclip(sample, sample.sentences[0], show_process=show_process, show_masks=show_masks, show=show, timeit=timeit)\n"
   ],
   "metadata": {
    "id": "pqzYing9Xlj_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 SSD + CLIP\n",
    "\n",
    "This approach combines CLIP's encoding power with the Single Shot Detection (SSD) algorithm from NVIDIA.\n",
    "\n",
    "\n",
    "SSD has two components: a backbone model and SSD head.\n",
    "\n",
    "- Backbone model usually is a pre-trained image classification network as a feature extractor.\n",
    "- The **SSD** head is just one or more convolutional layers added to this backbone and the outputs are interpreted as the bounding boxes and classes of objects in the spatial location of the final layers activations.\n",
    "\n",
    "SSD divides the image using a grid and have each grid cell be responsible for detecting objects in that region of the image. Detection objects simply means predicting the class and location of an object within that region. If no object is present, we consider it as the background class and the location is ignored. For instance, we could use a 4x4 grid in the example below. Each grid cell is able to output the position and shape of the object it contains.\n",
    "\n",
    "The pipeline uses SSD once again to detect relevant objects in the image, and then relies on CLIP for the actual grounding task, by comaring the detection similarity with the prompt."
   ],
   "metadata": {
    "collapsed": false,
    "id": "Quxw_pAQXlkA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the SSD + CLIP pipeline\n",
    "\n",
    "class ClipSSD(VisualGroundingPipeline):\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "\n",
    "        # Single Shot Detector (SSD) requires CUDA.\n",
    "        # Check if the selected device if CUDA before instantiating the class.\n",
    "\n",
    "        if kwargs[\"device\"] != torch.device(\"cuda\"):\n",
    "            print(\"[ERROR] Single Shot Detector requires CUDA. Returning empty object.\")\n",
    "            print(\"\")\n",
    "            return VisualGroundingPipeline.__new__(VisualGroundingPipeline)\n",
    "        else:\n",
    "            return super(ClipSSD, cls).__new__(cls)\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 confidence_t=0.5,\n",
    "                 clip_ver=\"ViT-L/14\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        self.confidence_t = confidence_t\n",
    "\n",
    "        self.ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\n",
    "        self.utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n",
    "\n",
    "        self.ssd_model.to(device)\n",
    "        self.ssd_model.eval()\n",
    "\n",
    "        print(\"[INFO] Initializing ClipSSD pipeline\")\n",
    "        print(f\"[INFO] Confidence treshold: {confidence_t}\")\n",
    "        print(\"\")\n",
    "\n",
    "    def _propose(self, image_path, original_size):\n",
    "\n",
    "        def _resize_bbox(bbox, in_size, out_size):\n",
    "            \"\"\"\n",
    "            Resize bounding boxes according to image resize.\n",
    "\n",
    "            Args:\n",
    "                bbox: (np.ndarray) bounding boxes of (y_min, x_min, y_max, x_max)\n",
    "                in_size: (tuple) the height and the width of the image before resized\n",
    "                out_size: (tuple) The height and the width of the image after resized\n",
    "            Returns:\n",
    "                (np.ndarray) bounding boxes rescaled according to the given image shapes\n",
    "\n",
    "            \"\"\"\n",
    "            bbox = bbox.copy()\n",
    "            y_scale = float(out_size[0]) / in_size[0]\n",
    "            x_scale = float(out_size[1]) / in_size[1]\n",
    "            bbox[:, 0] = y_scale * bbox[:, 0]\n",
    "            bbox[:, 2] = y_scale * bbox[:, 2]\n",
    "            bbox[:, 1] = x_scale * bbox[:, 1]\n",
    "            bbox[:, 3] = x_scale * bbox[:, 3]\n",
    "            return bbox\n",
    "\n",
    "        bboxes = []\n",
    "\n",
    "        inputs = [self.utils.prepare_input(image_path)]\n",
    "        tensor = self.utils.prepare_tensor(inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            detections_batch = self.ssd_model(tensor)\n",
    "\n",
    "        results_per_input = self.utils.decode_results(detections_batch)\n",
    "        best_results_per_input = [self.utils.pick_best(results, self.confidence_t) for results in results_per_input]\n",
    "\n",
    "        bbox, _, _ = best_results_per_input[0]\n",
    "        bbox *= 300\n",
    "        bbox = _resize_bbox(bbox, (300, 300), original_size)\n",
    "        bboxes.append(bbox)\n",
    "\n",
    "        return np.float32(bboxes[0]).tolist()\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=False, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        \"\"\" Pipeline core \"\"\"\n",
    "\n",
    "        # Get sample image\n",
    "        image_path = img_sample.path\n",
    "        img = img_sample.img\n",
    "\n",
    "        # Use SSD to propose relevant objects\n",
    "        bboxes = self._propose(image_path, (img_sample.shape[1], img_sample.shape[2]))\n",
    "\n",
    "        # Handle case where no object is proposed\n",
    "        if len(bboxes) == 0:\n",
    "            return {\"IoU\": 0, \"cosine\": np.nan, \"euclidean\": np.nan, \"dotproduct\": np.nan, \"grounding\": np.nan}\n",
    "\n",
    "        # Use CLIP to encode each relevant object detected\n",
    "        images_encs = list()\n",
    "        for bbox in bboxes:\n",
    "            sub_img = img.crop(bbox)\n",
    "            with torch.no_grad():\n",
    "                sub_img_enc = self._encode_img(sub_img)\n",
    "            images_encs.append(sub_img_enc)\n",
    "        images_encs = torch.cat(images_encs, dim=0)\n",
    "\n",
    "        # Use CLIP to encode the text prompt\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        # Find the best object according to cosine similarity\n",
    "        c_sims = cosine_similarity(prompt_enc, images_encs).squeeze()\n",
    "        best_idx = int(c_sims.argmax())\n",
    "\n",
    "        # Get best bbox\n",
    "        pred_bbox = bboxes[best_idx]\n",
    "\n",
    "        # Use CLIP to encode the prompt\n",
    "        prompt_enc = self._encode_text(prompt).float()\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T  # dot product\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)  # cosine similarity\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()  # euclidean distance\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            display_preds(img, prompt, pred_bbox, gt_bbox, model_name=\"SSD+CLIP\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "id": "nQG0tpVwXlkA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that said, we can instantiate and use an object for the SSD+CLIP pipeline."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ULvYXncaXlkA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate the SSD + CLIP pipeline\n",
    "\n",
    "# SSD + CLIP | with 0.01 confidence\n",
    "ssdnclip = ClipSSD(dataset.categories, confidence_t=0.01, device=device)\n"
   ],
   "metadata": {
    "id": "zQFC-BMwXlkA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the SSD + CLIP pipeline\n",
    "\n",
    "#@markdown Please flag `show` to show prediction, or `timeit` to time the process.<br>\n",
    "show = True #@param {type:\"boolean\"}\n",
    "timeit = False #@param {type:\"boolean\"}\n",
    "\n",
    "ssdnclip(sample, sample.sentences[0], show=show, timeit=timeit)\n"
   ],
   "metadata": {
    "id": "jL3dIpmyXlkA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 DETR + CLIP\n",
    "\n",
    "Unlike traditional computer vision techniques, **DETR (which stands for DEtection TRansformer)** approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. Due to this parallel nature, DETR is very fast and efficient.\n",
    "\n",
    "The DETR architecture basically consists of three blocks: (1) a set of CNN layers are used to extract features from the image, (2) the resulting vector - alongside a positional encoding - is then fed to a **Transformer with an Encoder-Decoder architecture**, and finally the output is forwarded to a **FF neural network**. The last layer consists of 3 nodes, representing the normalized center coordinates of the predicted object and the predicted height and width values of the bounding box detection.\n",
    "\n",
    "In this approach DETR is combined with CLIP to perform visual grounding. The DETR model is once again used to detect objects in the image, and CLIP is used to encode the text prompt and the objects detected. The object with the highest cosine similarity with the text prompt is selected as the grounding prediction."
   ],
   "metadata": {
    "collapsed": false,
    "id": "TOBtL1DtXlkB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for DETR + CLIP pipeline\n",
    "\n",
    "class DetrClip(VisualGroundingPipeline):\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 clip_ver=\"RN50\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "\n",
    "        logging.set_verbosity_error()\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        self.image_prep = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        self.detr = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "        print(\"[INFO] Initializing DetrClip pipeline\")\n",
    "        print(\"\")\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=False, show_detr=False, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        \"\"\" Pipeline core \"\"\"\n",
    "\n",
    "        # Get sample image\n",
    "        img = img_sample.img\n",
    "\n",
    "        # Make sure image has shape (h, w, 3)\n",
    "        np_image = np.array(img)\n",
    "        if len(np_image.shape) > 3 or (len(np_image.shape) == 3 and np_image.shape[-1] != 3):\n",
    "            np_image = np_image[:, :, 0]\n",
    "        if len(np_image.shape) == 2:\n",
    "            np_image = np.stack((np_image,) * 3, axis=-1)\n",
    "        img = Image.fromarray(np_image)\n",
    "\n",
    "        # Use DETR to find relevant objects\n",
    "        inputs = self.image_prep(images=img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.detr(**inputs)\n",
    "        target_sizes = torch.tensor([img_sample.img.size[::-1]])\n",
    "        results = self.image_prep.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "        detr_results = results['boxes']\n",
    "\n",
    "        # Use CLIP to encode each relevant object image\n",
    "        images_encs = list()\n",
    "        for i in range(detr_results.shape[0]):\n",
    "            bbox = results['boxes'][i, 0:4].cpu().numpy()\n",
    "            sub_img = img_sample.img.crop(bbox)\n",
    "            with torch.no_grad():\n",
    "                sub_img_enc = self._encode_img(sub_img)\n",
    "            images_encs.append(sub_img_enc)\n",
    "        images_encs = torch.cat(images_encs, dim=0)\n",
    "\n",
    "        # Use CLIP to encode the text prompt\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        # Find the best object according to cosine similarity\n",
    "        c_sims = cosine_similarity(prompt_enc, images_encs).squeeze()\n",
    "        best_idx = int(c_sims.argmax())\n",
    "\n",
    "        # Get best bbox\n",
    "        pred_bbox = detr_results[best_idx, 0:4].tolist()\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T  # dot product\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)  # cosine similarity\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()  # euclidean distance\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show objects found by DETR, if requested\n",
    "        if show_detr:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.imshow(img)\n",
    "            for i in range(detr_results.shape[0]):\n",
    "                bbox = results['boxes'][i].cpu().numpy()\n",
    "                # print(bbox)\n",
    "                rect = plt.Rectangle(\n",
    "                    (bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
    "                    linewidth=2, edgecolor=(0, 1, 0), facecolor=\"none\"\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "            ax.axis(\"off\")\n",
    "            plt.title(\"DETR findings\")\n",
    "            plt.show()\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            display_preds(img, prompt, pred_bbox, gt_bbox, model_name=f\"DETR + CLIP ({self.clip_ver})\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "id": "dDnPxA8VXlkB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now instantiate the pipeline and test it on a sample image and prompt."
   ],
   "metadata": {
    "collapsed": false,
    "id": "nfKBNrT_XlkB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate the DETR + CLIP pipeline\n",
    "\n",
    "# DETR + CLIP\n",
    "detrnclip = DetrClip(dataset.categories, quiet=True, device=device)\n"
   ],
   "metadata": {
    "id": "U1Axpvm-XlkC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the DETR + CLIP pipeline\n",
    "\n",
    "#@markdown Please flag `show` to show prediction, or `timeit` to time the process.<br>\n",
    "#@markdown Also check `show_detr` to display DETR findings.\n",
    "show = True #@param {type:\"boolean\"}\n",
    "timeit = False #@param {type:\"boolean\"}\n",
    "show_detr = True #@param {type:\"boolean\"}\n",
    "\n",
    "detrnclip(sample, sample.sentences[0], show_detr=True, show=show, timeit=timeit)\n"
   ],
   "metadata": {
    "id": "z3UBxdsMXlkC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 MDETR (SOTA)\n",
    "\n",
    "\n",
    "MDETR (which stands for Modulated Detection for End-to-End Multi-Modal Understanding) is a state-of-the-art object detection and instance segmentation model that combines image and textual information to perform tasks such as object detection and segmentation in an end-to-end manner.\n",
    "\n",
    "The MDETR model incorporates a transformer-based architecture, similar to those used in natural language processing tasks, to process both visual and textual data. It takes as input an image along with a textual query or question describing the desired objects to be detected. The model then processes the information in a joint manner, attending to both the visual and textual input, and produces bounding box predictions and segmentations for the specified objects in the image.\n",
    "\n",
    "It differs from traditional object detection models by eliminating the need for separate modules for region proposal and object classification. It directly attends to the entire image and textual query, enabling a more unified and efficient approach to multi-modal understanding tasks. The modality modulation mechanism in MDETR allows it to effectively handle different combinations of visual and textual inputs.\n",
    "\n",
    "In particular, MDETR uses the following components:\n",
    "\n",
    "- a **visual backbone** (RN101 in our code) to process the input image and to extract visual features\n",
    "- a **text encoder**, (RoBERTa in our code) which converts the textual input into contextualized embeddings\n",
    "- a **transformer encoder**, which receives the extracted embeddings alongside positional encodings for the visual features\n",
    "- finally, **detection heads**: which generate bounding box predictions and instance segmentations\n",
    "\n",
    "These components allow MDETR to combine image and textual information for end-to-end object detection and instance segmentation. The model attends to both visual and textual features simultaneously, enhancing its ability to understand and localize objects based on natural language queries. For this reason however it does not make use of CLIP, and it is reported here for the only purpose of showing the performances of a SOTA method in visual grounding."
   ],
   "metadata": {
    "collapsed": false,
    "id": "XAq5b96oXlkC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for MDETR\n",
    "\n",
    "class MDETRvg(VisualGroundingPipeline):\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 clip_ver=\"RN101\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        cpt_url = \"https://pytorch.s3.amazonaws.com/models/multimodal/mdetr/pretrained_resnet101_checkpoint.pth\"\n",
    "\n",
    "        self.MDETR = mdetr_for_phrase_grounding()\n",
    "        self.MDETR.load_state_dict(torch.hub.load_state_dict_from_url(cpt_url)[\"model_ema\"])\n",
    "        self.RoBERTa = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "        self.img_preproc = T.Compose([\n",
    "            T.Resize(800),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        print(\"[INFO] Initializing MDETR pipeline\")\n",
    "        print(\"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_boxes(boxes, size):\n",
    "        w, h = size\n",
    "        b = box_convert(boxes, \"cxcywh\", \"xyxy\")\n",
    "        b = b * torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "        return b\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=True, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        \"\"\" Pipeline core \"\"\"\n",
    "\n",
    "        # Get sample image\n",
    "        img = Image.open(img_sample.path)\n",
    "\n",
    "        # Make sure image has shape (h, w, 3)\n",
    "        np_image = np.array(img)\n",
    "        if len(np_image.shape) > 3 or (len(np_image.shape) == 3 and np_image.shape[-1] != 3):\n",
    "            np_image = np_image[:, :, 0]\n",
    "        if len(np_image.shape) == 2:\n",
    "            np_image = np.stack((np_image,) * 3, axis=-1)\n",
    "        img = Image.fromarray(np_image)\n",
    "\n",
    "        # Encode the prompt with RoBERTa\n",
    "        enc_text = self.RoBERTa.batch_encode_plus([prompt], padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "        # Preprocess the image for MDETR\n",
    "        img_transformed = self.img_preproc(img)\n",
    "\n",
    "        # Run MDETR on image and prompt\n",
    "        with torch.no_grad():\n",
    "            out = self.MDETR([img_transformed], enc_text[\"input_ids\"]).model_output\n",
    "\n",
    "        # Parse MDETR results to get detections bboxes and probabilities\n",
    "        probs = 1 - out.pred_logits.softmax(-1)[0, :, -1]\n",
    "        boxes_scaled = self.rescale_boxes(out.pred_boxes[0, :], img.size)\n",
    "        mdetr_results = pd.DataFrame(boxes_scaled.squeeze().numpy().reshape(-1, 4))\n",
    "        mdetr_results.columns = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "        mdetr_results[\"prob\"] = probs.numpy()\n",
    "        mdetr_results = mdetr_results.sort_values(by=['prob'], ascending=False)\n",
    "\n",
    "        # Get best bbox\n",
    "        pred_bbox = mdetr_results.iloc[0, :4].tolist()\n",
    "\n",
    "        # Use CLIP to encode the prompt\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            display_preds(img, prompt, pred_bbox, gt_bbox, model_name=\"MDETR\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "id": "vrjITVHlXlkC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that said, we can instantiate and use an object for the MDETR pipeline."
   ],
   "metadata": {
    "collapsed": false,
    "id": "A9EclzUoXlkC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate the MDETR pipeline\n",
    "\n",
    "# MDETR for visual grounding\n",
    "mdetr = MDETRvg(dataset.categories, quiet=True, device=device)\n"
   ],
   "metadata": {
    "id": "vqKTlt77XlkD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the MDETR pipeline\n",
    "\n",
    "#@markdown Please flag `show` to show prediction, or `timeit` to time the process.\n",
    "show = True #@param {type:\"boolean\"}\n",
    "timeit = True #@param {type:\"boolean\"}\n",
    "\n",
    "mdetr(sample, sample.sentences[0], show=show, timeit=timeit)\n"
   ],
   "metadata": {
    "id": "5y7vs9BxXlkD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Testing and results\n",
    "\n",
    "To test each visual grounding framework on the whole test split, a handy function is defined to run the pipeline on each sentence for each sample and compute the average metrics returned. The metrics are also printed in real time to monitor the progress of the testing process."
   ],
   "metadata": {
    "collapsed": false,
    "id": "NzauTDWOXlkD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Function to test a given visual grounding pipeline on a given dataset\n",
    "\n",
    "def visual_grounding_test(vg_pipeline, dataset, logging=False):\n",
    "    scores = list()\n",
    "\n",
    "    pbar = tqdm(dataset)\n",
    "\n",
    "    for sample in pbar:\n",
    "\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "\n",
    "            sc = vg_pipeline(sample, sentence, show=False)\n",
    "\n",
    "            scores.append(sc)\n",
    "\n",
    "            avg_metrics = list()\n",
    "\n",
    "            for metric in scores[0].keys():\n",
    "                avg_metric = np.mean([score[metric] for score in scores if score[metric] is not np.nan])\n",
    "                avg_metric = f\"{metric}: {avg_metric:.3f}\"\n",
    "                avg_metrics.append(avg_metric)\n",
    "\n",
    "            pbar_desc = \" | \".join(avg_metrics)\n",
    "\n",
    "            if logging:\n",
    "                pipeline_name = vg_pipeline.__class__.__name__.lower()\n",
    "                datetime_tag = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "                with open(f\"logs/{pipeline_name}_log_{datetime_tag}.txt\", \"a\") as f:\n",
    "                    f.write(\"[\" + datetime_tag + \"] \" + pbar_desc + \"\\n\")\n",
    "\n",
    "            pbar.set_description(pbar_desc)\n"
   ],
   "metadata": {
    "id": "GDL2GcvXXlkD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(You can use the dropdown menu to choose among one of the pipelines define above and test it right now. Still, notice the testing process can be pretty lenghty)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "G3NChPkxXlkD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test one of the pipeline defined\n",
    "\n",
    "pipeline = yolo5clip  #@param [\"yolo5clip\", \"yolo8clip\", \"wshedclip\", \"slicnclip\", \"detrnclip\", \"ssdnclip\", \"mdetr\"]\n",
    "\n",
    "visual_grounding_test(pipeline, test_ds)\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-04T12:21:07.053671Z",
     "end_time": "2023-06-04T12:21:07.136679Z"
    },
    "cellView": "form",
    "id": "RL28eZp7XlkD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Results discussion\n",
    "\n",
    "Here are reported the results achieved testing each approach on the test split of RefCOCOg.\n",
    "Note that the pipelines supporting a number of hyperparameters has been tested using the combination of parameters which achieved the best compromise in terms of performance and execution time."
   ],
   "metadata": {
    "collapsed": false,
    "id": "lJ6i7KcOXlkD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "|------------------------|------------------------|----------------------|----------------------|-----------------------|-----------------------|\n",
    "| Pipeline               | avg. IoU               | avg. cosine sim.     | avg. euclidean dist. | avg. dotproduct       | avg. grounding acc.   |\n",
    "|------------------------|------------------------|----------------------|----------------------|-----------------------|-----------------------|\n",
    "| YOLOv8x+CLIP(RN50)     | 0.554                  | 0.238                | 1.234                | 5.259                 | 0.353                 |\n",
    "| YOLOv8x+CLIP(RN101)    | 0.550                  | 0.459 *              | 1.040 *              | 8.842                 | 0.486                 |\n",
    "| YOLOv8x+CLIP(ViT-L/14) | 0.552                  | 0.254                | 1.221                | 0.254                 | 0.520 **              |\n",
    "| YOLOv5s+CLIP(RN50)     | 0.552                  | 0.238                | 1.234                | 0.238                 | 0.495                 |\n",
    "| YOLOv5s+CLIP(RN101)    | 0.550                  | 0.238                | 1.235                | 0.238                 | 0.496                 |\n",
    "| YOLOv5s+CLIP(ViT-L/14) | 0.533                  | 0.253                | 1.222                | 0.253                 | 0.515                 |\n",
    "| W.SHED+CLIP(ViT-L/14)  | 0.219                  | 0.243                | 1.230                | 0.243                 | 0.525 *               |\n",
    "| SLIC+CLIP(ViT-L/14)    | 0.180                  | 0.228                | 1.242                | 3.185                 | 0.373                 |\n",
    "| SSD+CLIP(RN50)         | 0.175                  | 0.217                | 1.251                | 2.319                 | 0.355                 |\n",
    "| SSD+CLIP(RN101)        | 0.172                  | 0.442                | 1.056                | 3.478                 | 0.365                 |\n",
    "| SSD+CLIP(ViT-L/14)     | 0.171                  | 0.225                | 1.245                | 3.138                 | 0.400                 |\n",
    "| DETR+CLIP(RN50)        | 0.560 **               | 0.237                | 1.235                | 0.237                 | 0.496                 |\n",
    "| DETR+CLIP(RN101)       | 0.547                  | 0.458 **             | 1.041 **             | 0.458                 | 0.482                 |\n",
    "| DETR+CLIP(ViT-L/14)    | 0.537                  | 0.252                | 1.223                | 0.252                 | 0.514                 |\n",
    "| MDETR (CLIPw/RN50)     | 0.617 *                | 0.225                | 1.244                | 0.225                 | 0.483                 |\n",
    "\n",
    "`*` = best <br>\n",
    "`**` = second best"
   ],
   "metadata": {
    "collapsed": false,
    "id": "LDsL8PBG1nfv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Considering the results achieved, we can draw some conclusions:\n",
    "\n",
    "Overall, succeeding into scoring a results better than the YOLO baseline has proven pretty hard. Over all the different approaches:\n",
    "\n",
    "- **The YOLO+CLIP baseline** retained its primate, at least in terms of semantic similarity (in particular with YOLOv8x and CLIP with a RN101 backbone)\n",
    "\n",
    "- **The DETR+CLIP approach** is a close runner up, with pretty similar metrics in terms of semantic similarity (again with a RN101 backbone for CLIP). The same approach (with a ResNet50 CLIP backbone) was also our second best framework in terms of localization accuracy.\n",
    "\n",
    "- **MDETR** shows all its SOTA potential, purely talking in terms of IoU, achieving the best results of all the tested pipelines.\n",
    "\n",
    "- **The Segmentation + CLIP approach** surely gets a honorable mention, achieving the best results in terms of grounding accuracy, even with unsatisfactory results in terms of IoU."
   ],
   "metadata": {
    "collapsed": false,
    "id": "Q4BIszhm1nfv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Future directions\n",
    "\n",
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2KAutQXsXlkE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
