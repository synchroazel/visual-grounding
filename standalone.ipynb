{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning Assignment 2023: Visual Grounding\n",
    "\n",
    "The goal of the project is to develop a deep learning framework to perform visual grounding on the RefCOCOg dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Throughout the work multiple approaches have been tested, yielding some very different results and with import differences in terms of performance. For the sake of the providing a complete overview of the work done, the notebook will go through all of the approach tested, presenting any idea that has been discussed and implemented in these months of work, discussing their pros and limitations. The approaches that have been taken into account include:\n",
    "\n",
    "- a baseline pipeline using YOLO for object proposals and CLIP for the grounding task;\n",
    "- a pipeline which involves image segmentation, CLIP embedding and bounding box proposal;\n",
    "- a pipeline using DETR for object proposals and CLIP for the grounding task;\n",
    "- a pipeline only using MDETR for the grounding task (for the sake of having a SOTA comparison)\n",
    "- a framework involving Diffusion for bounding box detection and CLIP for the grounding task.\n",
    "- a framework involving Reinforcement Learning for bounding box regression.\n",
    "\n",
    "The notebook will cover all aforementioned approaches,\n",
    "\n",
    "- (1) starting from a brief introduction, covering (i) the problem of visual grounding, (ii) a quick presentation of the dataset and (iii) CLIP\n",
    "- (2) continuing through the implementation of the different frameworks, discussing pro and cons\n",
    "- (3) and finally presenting the overall results achieved and providing some final considerations.\n",
    "\n",
    "The notebook is meant to be run standalone here on Google Colab but, may it be useful, the codebase is also available on GitHub at [synchroazel/visual-grounding](https://github.com/synchroazel/visual-grounding). Please refer to the README for further instructions on how to run it locally."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Brief introduction\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Visual Grounding\n",
    "\n",
    "Visual grounding refers to the process of linking visual information with corresponding linguistic symbols, bridging the gap between visual perception and linguistic understanding. The goal is to enable machines to comprehend and interpret visual information in a way that is similar to human understanding.\n",
    "\n",
    "Concretely, visual grounding can be approached in different ways, depending on the specific task and context. One common approach is to use techniques like object detection, image segmentation, or scene understanding to extract relevant visual features from an image or video. These features are then matched or aligned with corresponding textual descriptions or concepts using a multitude of approaches.\n",
    "\n",
    "Many of the current approaches to visual grounding can be identified either as a **one-stage** or **two-stage**.\n",
    "\n",
    "- Two-stage methods formulate visual grounding as a matching problem between language and region. The visual region proposals are extracted by a pre-trained detector in the first stage, which are matched with the given expression in the second stage. However, the performance of these methods is highly dependent on the detector in the first stage. Besides, matching must be performed for every region proposals, which drag a great extent on the speed of the network.\n",
    "\n",
    "- One-stage methods overcome the reliance on detectors and speed up the inference process by grounding the object in an image by a sentence query directly.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 RefCOCOg Dataset\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Contrastive Language-Image Pre-Training (CLIP)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminary steps\n",
    "\n",
    "Here are some preliminary steps before we start discussing each approach in more detail."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-04T12:29:13.473538Z",
     "end_time": "2023-06-04T12:29:13.581217Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import all necessary modules\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import clip\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    "from skimage.measure import regionprops\n",
    "from skimage.segmentation import slic, watershed\n",
    "from skimage.util import img_as_float\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torchmultimodal.models.mdetr.model import mdetr_for_phrase_grounding\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.ops.boxes import box_convert\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizerFast\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from modules.pipelines.clipssd import ClipSSD\n",
    "from modules.pipelines.detrclip import DetrClip\n",
    "from modules.pipelines.vgpipeline import VisualGroundingPipeline\n",
    "from modules.utilities import cosine_similarity\n",
    "from modules.utilities import display_preds, downsample_map\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Set the best device for to the machine running the notebook\n",
    "\n",
    "def get_best_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device_ = torch.device(\"cuda\")  # for CUDA GPU\n",
    "        print(\"[INFO] Using cuda.\")\n",
    "    elif torch.has_mps:\n",
    "        device_ = torch.device(\"mps\")  # for Apple Silicon GPU\n",
    "        print(\"[INFO] Using MPS.\")\n",
    "    else:\n",
    "        device_ = torch.device(\"cpu\")\n",
    "        print(\"[INFO] No GPU found, using CPU instead.\")\n",
    "\n",
    "    return device_\n",
    "\n",
    "\n",
    "device = get_best_device()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-04T12:17:24.647965Z",
     "end_time": "2023-06-04T12:17:24.693540Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is also important to define our dataset and sample objects, which are going to be used throughout the notebook.\n",
    "\n",
    "- The `RefCOCOgSample` object define a single sample from the RefCOCOg dataset, containing all the information - as attributes - that may be needed for visual grounding tasks.\n",
    "\n",
    "- The `RefCOCOg` object is a wrapper around the RefCOCOg dataset, providing some useful methods to access the dataset and its samples. Most notably, it implements (1) a `__getitem__` method to access a single sample and (2) a `__len__` method to get the number of samples in the dataset. Also notice that the `__getitem__` method returns a dictionary containing the information that can be used to populate a `RefCOCOgSample` object (simply with `RefCOCOgSample(**output)`). The `__init__` method also takes care of loading the correct split we might need, accepting either `train`, `val` or `test` as arguments and using the annotations from `refs(umd).p` to select the samples of each."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for a sample from the RefCOCOg dataset\n",
    "\n",
    "class RefCOCOgSample:\n",
    "    \"\"\"\n",
    "    An annotated image from RefCOCOg dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 img: Image.Image,\n",
    "                 shape: tuple[int, int],\n",
    "                 path: str,\n",
    "                 img_id: str,\n",
    "                 split: str,\n",
    "                 category: str,\n",
    "                 category_id: int,\n",
    "                 sentences: list[str],\n",
    "                 bbox: list[float],\n",
    "                 segmentation: list[float]):\n",
    "        self.img = img\n",
    "        self.shape = shape\n",
    "        self.path = path\n",
    "        self.id = img_id\n",
    "        self.split = split\n",
    "        self.category = category\n",
    "        self.category_id = category_id\n",
    "        self.sentences = sentences\n",
    "        self.bbox = bbox\n",
    "        self.segmentation = segmentation\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(vars(self))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the RefCOCOg dataset\n",
    "\n",
    "class RefCOCOg(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object for RefCOCOg dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds_path: str, split=None, transform=None):\n",
    "        super(RefCOCOg, self).__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.ds_path = ds_path\n",
    "\n",
    "        with open(f'{ds_path}/annotations/refs(umd).p', 'rb') as f:\n",
    "            self.refs = pickle.load(f)\n",
    "\n",
    "        with open(f\"{ds_path}/annotations/instances.json\", \"r\") as f:\n",
    "            self.instances = json.load(f)\n",
    "\n",
    "        self.categories = {\n",
    "            item[\"id\"]: {\n",
    "                \"supercategory\": item[\"supercategory\"],\n",
    "                \"category\": item[\"name\"]\n",
    "            }\n",
    "            for item in self.instances['categories']\n",
    "        }\n",
    "\n",
    "        if split == 'train':\n",
    "            self.refs = [ref for ref in self.refs if ref['split'] == 'train']\n",
    "        elif split == 'val':\n",
    "            self.refs = [ref for ref in self.refs if ref['split'] == 'val']\n",
    "        elif split == 'test':\n",
    "            self.refs = [ref for ref in self.refs if ref['split'] == 'test']\n",
    "\n",
    "        self.size = len(self.refs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        refs_data = self.refs[idx]\n",
    "\n",
    "        for inst in self.instances['annotations']:\n",
    "            if inst['id'] == refs_data['ann_id']:\n",
    "                ann_data = inst\n",
    "                break\n",
    "\n",
    "        image_path = os.path.join(\n",
    "            self.ds_path,\n",
    "            \"images\",\n",
    "            re.sub(r\"_[0-9]+\\.jpg\", \".jpg\", refs_data[\"file_name\"])\n",
    "        )\n",
    "\n",
    "        pil_img = Image.open(image_path)\n",
    "\n",
    "        bbox = torch.tensor(ann_data[\"bbox\"])\n",
    "        bbox = box_convert(bbox, \"xywh\", \"xyxy\").numpy()\n",
    "\n",
    "        sample = {\n",
    "            \"img\": pil_img,\n",
    "            \"shape\": transforms.ToTensor()(pil_img).shape,\n",
    "            \"path\": image_path,\n",
    "            \"img_id\": refs_data[\"image_id\"],\n",
    "            \"split\": refs_data[\"split\"],\n",
    "            \"category\": self.categories[refs_data[\"category_id\"]][\"category\"],\n",
    "            \"category_id\": refs_data[\"category_id\"],\n",
    "            \"sentences\": [sentence[\"raw\"].lower() for sentence in refs_data[\"sentences\"]],\n",
    "            \"bbox\": bbox,\n",
    "            \"segmentation\": ann_data[\"segmentation\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample[\"img\"], dtype=torch.float32)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that taken care of, we can now import the dataset and its split."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Import the RefCOCOg dataset and create train/validation/test splits\n",
    "\n",
    "data_path = \"dataset/refcocog\"\n",
    "\n",
    "dataset = RefCOCOg(ds_path=data_path)\n",
    "\n",
    "train_ds = RefCOCOg(ds_path=data_path, split='train')\n",
    "val_ds = RefCOCOg(ds_path=data_path, split='val')\n",
    "test_ds = RefCOCOg(ds_path=data_path, split='test')\n",
    "\n",
    "print(f\"[INFO] Dataset Size: {len(dataset)}\")\n",
    "print(f\"[INFO] train split:  {len(train_ds)}\")\n",
    "print(f\"[INFO] val split:    {len(val_ds)}\")\n",
    "print(f\"[INFO] test split:   {len(test_ds)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-04T12:17:24.693787Z",
     "end_time": "2023-06-04T12:17:35.811084Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. The pipelines\n",
    "\n",
    "In the following section we are going to present the different visual grounding pipelines that have been implemented and tested."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**About the metrics**\n",
    "\n",
    "Before diving into each approach, it is worth mentioning the metrics that have been used to evaluate the performance of each pipeline. The metrics used aim to evaluate the performance of the framework in terms of\n",
    "\n",
    "(1) localization accuracy\n",
    "(2) grounding accuracy\n",
    "(3) semantic similarity\n",
    "\n",
    "Localization accuracy refers to the ability of the model to localize an object in the image, and it is measured using **Intersection over Union (IoU)**, namely the ratio between the area of overlap between the predicted bounding box and the ground-truth bounding box and the area of union between the two.\n",
    "\n",
    "Semantic similarity refers to the similarity between the predicted bounding boxes and the ground-truth descriptions, and it is measured using distance metrics such as cosine similarity, **euclidean distance** or **dot product** between the CLIP embeddings of the predicted bounding boxes and the ground-truth textual description.\n",
    "\n",
    "Grounding accuracy refers to the ability of the model to ground the localized object to a language description, and it is measured using **recall**, namely the ratio between the number of correctly grounded objects and the total number of objects. Practically\n",
    "\n",
    "- we get a CLIP encoding of each available category in a dummy `f\"a picture of a {object}\"` sentence\n",
    "- we get a CLIP encoding of the bounding boxed image proposed by the pipeline\n",
    "- we compare those and get the category that is most similar\n",
    "- if the category is the same as the one in the ground truth, we have a correct grounding attempt\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**About the implementation**\n",
    "\n",
    "To simplify our codebase, and to provide extra readability, we used a common superclass `VisualGroundingPipeline` for all the visual grounding frameworks approached. This superclass provides a common interface for all the pipelines, such as\n",
    "\n",
    "- (1) initialization of common attributes\n",
    "- (2) a pair of methods to encode text and images using CLIP after taking care of tokenization/preprocess\n",
    "- (3) a method to compute IoU\n",
    "- (4) a method to compute the grounding accuracy and finally a method used to embed, on pipeline instantiation, all the available categories (useful, as mentioned, for the visual grounding accuracy computation).\n",
    "\n",
    "All pipelines inherit from this superclass, and implement one or more custom methods, most notably one of them being a `__call__` method which contains the core logic and returns the metrics of interest.\n",
    "\n",
    "At subclass level each pipeline also implement a set of common attributes to be used in the `__call__` method for displaying and testing purposes, namely\n",
    "\n",
    "- `show=True` will display the image with the predicted bounding box\n",
    "- `timeit=True` will print the time taken to run the pipeline (it is recommended to use it without any visualization)\n",
    "-"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the VisualGroundingPipeline superclass\n",
    "\n",
    "class VisualGroundingPipeline:\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 clip_ver=\"RN50\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "        self.categories = copy.deepcopy(categories)\n",
    "        self.clip_ver = clip_ver\n",
    "        self.clip_model, self.clip_prep = clip.load(clip_ver, device=\"cpu\")\n",
    "        self.device = device\n",
    "        self.quiet = quiet\n",
    "\n",
    "        # model is loaded to cpu first, and eventually moved to gpu\n",
    "        # (trick Mac M1 to use f16 tensors)\n",
    "        if self.device != \"cpu\":\n",
    "            self.clip_model = self.clip_model.to(self.device)\n",
    "\n",
    "        self._embed_categories()\n",
    "\n",
    "    def _encode_text(self, text):\n",
    "        text_ = clip.tokenize(text).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self.clip_model.encode_text(text_)\n",
    "\n",
    "    def _encode_img(self, image):\n",
    "        image_ = self.clip_prep(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self.clip_model.encode_image(image_)\n",
    "\n",
    "    @staticmethod\n",
    "    def _IoU(pred_bbox, gt_bbox):\n",
    "        iou = box_iou(\n",
    "            torch.tensor(pred_bbox).unsqueeze(0),\n",
    "            torch.tensor(gt_bbox).unsqueeze(0)\n",
    "        ).item()\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def _grounding_accuracy(self, img_sample, pred_image_enc):\n",
    "        all_c_sims = dict()\n",
    "\n",
    "        for category_id in self.categories.keys():\n",
    "            cur_categ = self.categories[category_id]['category']\n",
    "            cur_categ_enc = self.categories[category_id]['encoding'].float()\n",
    "\n",
    "            all_c_sims[cur_categ] = cosine_similarity(pred_image_enc, cur_categ_enc)\n",
    "\n",
    "        pred_category = max(all_c_sims, key=all_c_sims.get)\n",
    "\n",
    "        # if not self.quiet:\n",
    "        #     print(f\"[INFO] true: {img_sample.category} | predicted: {pred_category}\")\n",
    "\n",
    "        return 1 if pred_category == img_sample.category else 0\n",
    "\n",
    "    def _embed_categories(self):\n",
    "        for category_id in self.categories.keys():\n",
    "            cur_category = self.categories[category_id]['category']\n",
    "            with torch.no_grad():\n",
    "                cur_category_enc = self._encode_text(f\"a photo of {cur_category}\")\n",
    "            self.categories[category_id].update({\"encoding\": cur_category_enc})\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, before skipping ahead, we pick a random a sample image from the dataset to use as a quick test for the following sections."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Pick a random sample from the dataset\n",
    "\n",
    "idx = np.random.randint(0, len(dataset))\n",
    "sample = RefCOCOgSample(**dataset[idx])\n",
    "\n",
    "plt.imshow(sample.img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Sample #{idx}\", loc=\"left\")\n",
    "plt.show()\n",
    "\n",
    "for i, sentence in enumerate(sample.sentences):\n",
    "    print(f\"[INFO] Sentence #{i}: {sentence}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 YOLO + CLIP\n",
    "\n",
    "The first approach that has been tested is a baseline pipeline using YOLO for object proposals and CLIP for the grounding task. The idea is to use YOLO to extract the bounding boxes of the objects in an image and then use CLIP to find the best match between the bounding boxes and the sentence query. This latter part is simply done by computing CLIP embeddings of the cropped bounding boxes and the sentence query and then computing the cosine similarity between the two embeddings. The bounding box with the highest similarity score is then selected as the best match.\n",
    "\n",
    "**YOLO**, which funnily stands for \"You Only Look Once,\" is a popular object detection model in computer vision. It revolutionized real-time object detection by proposing a unified framework that simultaneously predicts object bounding boxes and class probabilities in a single pass. YOLO divides the input image into a grid and applies convolutional neural networks to each grid cell to predict bounding boxes and class probabilities. This approach allows YOLO to achieve impressive detection speeds while maintaining competitive accuracy.\n",
    "\n",
    "This approach has been tested both using different version of YOLO (YOLOv8x and YOLOv5su - which recently replaced YOLOv5s) and different visual backbones for CLIP (ResNet50, ResNet101 and ViT-L/14). The results are shown in the following table."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the YOLO+Clip pipeline\n",
    "\n",
    "class YoloClip(VisualGroundingPipeline):\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 yolo_ver=\"yolov8x\",\n",
    "                 clip_ver=\"RN50\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        self.yolo_ver = yolo_ver\n",
    "        self.yolo_model = YOLO(self.yolo_ver + \".pt\")\n",
    "\n",
    "        valid_yolo_versions = [\"yolov8x\", \"yolov5su\"]\n",
    "        if yolo_ver not in valid_yolo_versions:\n",
    "            raise ValueError(f\"Invalid YOLO version '{yolo_ver}'. Must be one of {valid_yolo_versions}.\")\n",
    "\n",
    "        print(\"[INFO] Initializing YoloClip pipeline\")\n",
    "        print(f\"[INFO] YOLO version: {yolo_ver}\")\n",
    "        print(\"\")\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=False, show_yolo=False, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        # Get sample image\n",
    "        img = img_sample.img\n",
    "\n",
    "        # Use YOLO to propose relevant objects\n",
    "        yolo_results_ = self.yolo_model(img_sample.path, verbose=False)[0]\n",
    "        yolo_results = yolo_results_.boxes.xyxy\n",
    "        if not self.quiet:\n",
    "            print(f\"[INFO] YOLO found {yolo_results.shape[0]} objects\")\n",
    "        if yolo_results.shape[0] == 0:\n",
    "            print(f\"[WARN] YOLO ({self.yolo_ver}) couldn't find any object in {img_sample.path}!\")\n",
    "            return {\"IoU\": 0, \"cosine\": np.nan, \"euclidean\": np.nan, \"dotproduct\": np.nan, \"grounding\": np.nan}\n",
    "\n",
    "        # Use CLIP to encode each relevant object image\n",
    "        images_encs = list()\n",
    "        for i in range(yolo_results.shape[0]):\n",
    "            bbox = yolo_results[i, 0:4].cpu().numpy()\n",
    "            sub_img = img.crop(bbox)\n",
    "            with torch.no_grad():\n",
    "                sub_img_enc = self._encode_img(sub_img)\n",
    "            images_encs.append(sub_img_enc)\n",
    "        images_encs = torch.cat(images_encs, dim=0)\n",
    "\n",
    "        # Use CLIP to encode the text prompt\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        # Compute the best bbox according to cosine similarity\n",
    "        c_sims = cosine_similarity(prompt_enc, images_encs).squeeze()\n",
    "        best_idx = int(c_sims.argmax())\n",
    "\n",
    "        # Get best bbox\n",
    "        pred_bbox = yolo_results[best_idx, 0:4].tolist()\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T  # dot product\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)  # cosine similarity\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()  # euclidean distance\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show objects found by YOLO, if requested\n",
    "        if show_yolo:\n",
    "            plt.imshow(yolo_results_.plot())\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"YOLO findings\")\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            display_preds(img, prompt, pred_bbox, gt_bbox, model_name=f\"{self.yolo_ver} + CLIP({self.clip_ver})\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that said, we can instantiate two object for the YOLO+CLIP pipeline, using different YOLO versions, and quickly see them running."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate YOLO+CLIP pipelines\n",
    "\n",
    "# YOLOv5 + CLIP\n",
    "yolo5clip = YoloClip(dataset.categories, yolo_ver=\"yolov5su\", quiet=True, device=device)\n",
    "\n",
    "# YOLOv8 + CLIP\n",
    "yolo8clip = YoloClip(dataset.categories, yolo_ver=\"yolov8x\", quiet=True, device=device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-04T12:17:35.812635Z",
     "end_time": "2023-06-04T12:18:13.701099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the YOLOv5 + CLIP pipeline\n",
    "\n",
    "yolo5clip(sample, sample.sentences[0], show_yolo=True, show=True, timeit=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the YOLOv8 + CLIP pipeline\n",
    "\n",
    "yolo8clip(sample, sample.sentences[0], show_yolo=True, show=True, timeit=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Segmentation + CLIP\n",
    "\n",
    "This approach is fundamentally based on the idea of segmenting the sample image into a set of regions and interpreting the similarity of each section to the prompt as a heatmap. An heatmap in this context is obtained as follows:\n",
    "\n",
    "1. The sample image is segmented using a segmentation algorithm (e.g. SLIC, Watershed)\n",
    "2. Each region is encoded using CLIP\n",
    "3. The similarity between the prompt and each region is computed\n",
    "4. Each \"pixel\" inside each region is assigned the similarity score of that region, producing a score heatmap\n",
    "\n",
    "That being said, simply computing one single heatmap may not be enough to capture both larger and fine-grained image-text similarites. For such reasons, we compute different heatmaps using a different number of segments each time, and finally pooling all the heatmap together using the mean score of each pixel. This is not all though. At this point:\n",
    "\n",
    "1. All pixels below a certain threshold are turned off (set to 0)\n",
    "2. The heatmap is downsampled with a certain factor for lighter computations\n",
    "3. The heatmap is normalized to the range [-1, 1]\n",
    "4. The best bounding box is extracted from the heatmap, by considering all possible bounding boxes and selecting the one with the highest pixel sum\n",
    "\n",
    "Note that:\n",
    "- the threshold used to filter out pixels is taken as the value of a certain quantile `q` of the pixel values distribution\n",
    "- the bbox search algorithm searches over all possible boxes, and that is why the heatmap is downsampled first\n",
    "- the normalization also serves to give turned-off pixels negative values and discourage the algorithm from selecting areas containing many\n",
    "\n",
    "Although being pretty flexible in terms of experimentation with hyperparameters, this model suffers from a few limitations, one of them being the slow inference time. This could be definitely improved in the future by defining another bounding box search algorithm, such as [...].\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Some implementative details***\n",
    "\n",
    "The class implements a `__compute_hmap` method, which is responsible for computing the heatmap by passing a method and a number of segments to use it with. The method is used by `__call__` which performs the other steps described above and returns the metrics. The `_find_best_bbox` method is responsible for bbox searching, as decribed.\n",
    "\n",
    "When calling the pipeline, the user can specify two additional parameters:\n",
    "\n",
    "- `show_process=True` will show the resulting heatmap alongside its filtered and downsampled versions\n",
    "- `show_masks=True` will show all *N* masks before pooling, with *N* being the number of segments used\n",
    "\n",
    "Also, note that the hyperparameters specified in the class instantiation below are those which, experimentally, gave the best results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the Segmentation + CLIP pipeline\n",
    "\n",
    "class ClipSeg(VisualGroundingPipeline):\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 method,\n",
    "                 n_segments,\n",
    "                 clip_ver=\"ViT-L/14\",\n",
    "                 q=0.95,\n",
    "                 d=16,\n",
    "                 device=\"cpu\",\n",
    "                 quiet=False):\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        self.method = method\n",
    "        self.n_segments = n_segments\n",
    "        self.q = q\n",
    "        self.d = d\n",
    "\n",
    "        valid_methods = [\"s\", \"w\"]\n",
    "        if self.method not in valid_methods:\n",
    "            raise ValueError(f\"Method `{method}` not supported. Supported methods are: {valid_methods}.\")\n",
    "\n",
    "        print(\"[INFO] Initializing ClipSeg pipeline\")\n",
    "        print(f\"[INFO] Segmentation method: {method}\")\n",
    "        print(f\"[INFO] Number of segments: {n_segments}\")\n",
    "        print(f\"[INFO] Threshold q.tile for filtering: {q}\")\n",
    "        print(f\"[INFO] Downsampling factor: {d}\")\n",
    "        print(\"\")\n",
    "\n",
    "    def _compute_hmap(self, img_sample, np_image, prompt, method, masks):\n",
    "\n",
    "        # Make sure np_image is an image with shape (h, w, 3)\n",
    "        if len(np_image.shape) > 3 or (len(np_image.shape) == 3 and np_image.shape[-1] != 3):\n",
    "            np_image = np_image[:, :, 0]\n",
    "\n",
    "        if len(np_image.shape) == 2:\n",
    "            np_image = np.stack((np_image,) * 3, axis=-1)\n",
    "\n",
    "        hmaps = list()\n",
    "\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        for i, n in enumerate(masks):\n",
    "\n",
    "            # Compute regions according to chosen method\n",
    "            segments = None\n",
    "            if method == \"s\":\n",
    "                # SLIC segmentation algorithm ()\n",
    "                segments = slic(np_image, n_segments=n, compactness=10, sigma=1)\n",
    "            elif method == \"w\":\n",
    "                # Watershed segmentation algorithm ()\n",
    "                segments = watershed(sobel(rgb2gray(np_image)), markers=n, compactness=0.001)\n",
    "\n",
    "            if segments is None:\n",
    "                raise Exception(\"Segments are None. Is method different from 's' or 'w'? \")\n",
    "\n",
    "            regions = regionprops(segments)\n",
    "\n",
    "            if len(regions) == 1:\n",
    "                # If the algo returned only 1 region, skip this iteration\n",
    "                # (may happen, with low-segments masks)\n",
    "                continue\n",
    "\n",
    "            # Compute CLIP encodings for each region\n",
    "\n",
    "            images_encs = list()\n",
    "\n",
    "            regions = tqdm(regions, desc=f\"[INFO] Computing CLIP masks\", leave=False) if not self.quiet else regions\n",
    "\n",
    "            for region in regions:\n",
    "                rect = region.bbox\n",
    "                rect = (rect[1], rect[0], rect[3], rect[2])\n",
    "\n",
    "                sub_image = img_sample.img.crop(rect)\n",
    "                image_enc = self._encode_img(sub_image)\n",
    "                images_encs.append(image_enc)\n",
    "\n",
    "            # Assign a score to each region according to prompt similarity (creating a heatmap)\n",
    "\n",
    "            images_encs = torch.cat(images_encs, dim=0)\n",
    "            scores = prompt_enc @ images_encs.T\n",
    "            scores = scores.squeeze().cpu().numpy()\n",
    "            heatmap = np.zeros((segments.shape[0], segments.shape[1]))\n",
    "\n",
    "            for i in range(segments.shape[0]):\n",
    "                for j in range(segments.shape[1]):\n",
    "                    heatmap[i, j] = scores[segments[i, j] - 1]\n",
    "\n",
    "            hmaps.append(heatmap)\n",
    "\n",
    "        # Finally, return the pooled heatmap and the list of all heatmaps computed\n",
    "\n",
    "        pmap = np.mean(np.array(hmaps), axis=0)\n",
    "\n",
    "        return pmap, hmaps\n",
    "\n",
    "    def _find_best_bbox(self, heatmap, lower_bound=-1.0, upper_bound=1.0):\n",
    "        # Rescale the heatmap\n",
    "        heatmap = MinMaxScaler(feature_range=(lower_bound, upper_bound)).fit_transform(heatmap)\n",
    "\n",
    "        # Initialize the best score and best box\n",
    "        best_score = float('-inf')\n",
    "        best_box = None\n",
    "\n",
    "        # Loop over all possible box sizes and positions\n",
    "        for w in range(1, heatmap.shape[1] + 1):\n",
    "            for h in range(1, heatmap.shape[0] + 1):\n",
    "                for i in range(heatmap.shape[1] - w + 1):\n",
    "                    for j in range(heatmap.shape[0] - h + 1):\n",
    "\n",
    "                        # Get current sub-region\n",
    "                        candidate = heatmap[j:j + h, i:i + w]\n",
    "\n",
    "                        # Compute the score for this box\n",
    "                        score = candidate.sum()\n",
    "\n",
    "                        # Update the best score and best box if necessary\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_box = (i, j, w, h)\n",
    "\n",
    "        best_box = [best_box[0], best_box[1], best_box[2] + best_box[0], best_box[3] + best_box[1]]\n",
    "\n",
    "        return best_box\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=False, show_process=False, show_masks=False, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        \"\"\" Pipeline core \"\"\"\n",
    "\n",
    "        # Get sample image\n",
    "        img = img_sample.img\n",
    "\n",
    "        # Convert image to np array\n",
    "        np_image = img_as_float(io.imread(img_sample.path))\n",
    "\n",
    "        # Compute an heatmap of CLIP scores\n",
    "        p_heatmap, heatmaps = self._compute_hmap(img_sample, np_image, prompt, self.method, self.n_segments)\n",
    "\n",
    "        # Shut down pixels below a certain threshold\n",
    "        ths = np.quantile(p_heatmap.flatten(), self.q)\n",
    "        fp_heatmap = p_heatmap.copy()\n",
    "        fp_heatmap[p_heatmap < ths] = ths\n",
    "\n",
    "        # Downsample the heatmap by a factor d\n",
    "        dfp_heatmap = downsample_map(fp_heatmap, self.d)\n",
    "\n",
    "        # Find the best bounding box\n",
    "        pred_bbox = self._find_best_bbox(dfp_heatmap, lower_bound=-0.75)\n",
    "\n",
    "        if pred_bbox is None:\n",
    "            return {\"IoU\": 0, \"cosine\": np.nan, \"euclidean\": np.nan, \"dotproduct\": np.nan, \"grounding\": np.nan}\n",
    "\n",
    "        if self.d > 1:\n",
    "            pred_bbox = [pred_bbox[0] * self.d + self.d // 2,\n",
    "                         pred_bbox[1] * self.d + self.d // 2,\n",
    "                         pred_bbox[2] * self.d - self.d // 2,\n",
    "                         pred_bbox[3] * self.d - self.d // 2]\n",
    "\n",
    "        # Use CLIP to encode the text prompt\n",
    "        prompt_enc = self._encode_text(prompt).float()\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T  # dot product\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)  # cosine similarity\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()  # euclidean distance\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show all masks, if requested\n",
    "        if show_masks:\n",
    "            fig, axes = plt.subplots(1, len(heatmaps), figsize=(20, 5))\n",
    "            for i, heatmap in enumerate(heatmaps):\n",
    "\n",
    "                for ax in axes.ravel():\n",
    "                    ax.axis(\"off\")\n",
    "\n",
    "                axes[i].imshow(np_image, alpha=0.25)\n",
    "                axes[i].imshow(heatmap, alpha=0.75)\n",
    "                axes[i].set_title(f\"#{i + 1}\")\n",
    "\n",
    "        # Show the mask processing pipeline, if requested\n",
    "        if show_process:\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "            for ax in axes.ravel():\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            axes[0].imshow(np_image)\n",
    "            axes[0].set_title(\"original image\")\n",
    "\n",
    "            axes[1].imshow(np_image, alpha=0.25)\n",
    "            axes[1].imshow(p_heatmap, alpha=0.75)\n",
    "            axes[1].set_title(\"pooled heatmap\")\n",
    "\n",
    "            axes[2].imshow(np_image, alpha=0.25)\n",
    "            axes[2].imshow(fp_heatmap, alpha=0.75)\n",
    "            axes[2].set_title(\"filtered heatmap\")\n",
    "\n",
    "            axes[3].imshow(np_image, alpha=0.25)\n",
    "            w, h = np_image.shape[1], np_image.shape[0]\n",
    "            dfp_heatmap_ = cv2.resize(dfp_heatmap, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "            axes[3].imshow(dfp_heatmap_, alpha=0.75)\n",
    "            axes[3].set_title(\"dsampled heatmap\")\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            methods = {\"w\": \"Watershed\", \"s\": \"SLIC\"}\n",
    "            display_preds(img_sample.img, prompt, pred_bbox, img_sample.bbox,\n",
    "                          f\"{methods[self.method]} + CLIP ({self.clip_ver})\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that said, we can instantiate two object for the Segmentation+CLIP pipeline, which we will conveniently call after the segmentation method used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate the segmentation + CLIP pipelines\n",
    "\n",
    "# Watershed seg. + CLIP | pooling maps with 4, 8, 16, 32 segments | filtering below 0.75 q.tile\n",
    "wshedclip = ClipSeg(dataset.categories, method=\"w\", n_segments=(4, 8, 16, 32), q=0.75, quiet=False, device=device)\n",
    "\n",
    "# SLIC seg. + CLIP | pooling maps with 4, 8, 16, 32 segments | filtering below 0.75 q.tile\n",
    "slicnclip = ClipSeg(dataset.categories, method=\"s\", n_segments=(4, 8, 16, 32), q=0.75, quiet=False, device=device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the Watershed + CLIP pipeline\n",
    "\n",
    "wshedclip(sample, sample.sentences[0], show_process=True, show_masks=True, show=True, timeit=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the SLIC + CLIP pipeline\n",
    "\n",
    "slicnclip(sample, sample.sentences[0], show_process=True, show_masks=True, show=True, timeit=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 SSD + CLIP\n",
    "\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for the SSD + CLIP pipeline\n",
    "\n",
    "class ClipSSD(VisualGroundingPipeline):\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "\n",
    "        # Single Shot Detector (SSD) requires CUDA.\n",
    "        # Check if the selected device if CUDA before instantiating the class.\n",
    "\n",
    "        if kwargs[\"device\"] != torch.device(\"cuda\"):\n",
    "            print(\"[ERROR] Single Shot Detector requires CUDA. Returning empty object.\")\n",
    "            print(\"\")\n",
    "            return VisualGroundingPipeline.__new__(VisualGroundingPipeline)\n",
    "        else:\n",
    "            return super(ClipSSD, cls).__new__(cls)\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 confidence_t=0.5,\n",
    "                 clip_ver=\"ViT-L/14\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        self.confidence_t = confidence_t\n",
    "\n",
    "        self.ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\n",
    "        self.utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n",
    "\n",
    "        self.ssd_model.to(device)\n",
    "        self.ssd_model.eval()\n",
    "\n",
    "        print(\"[INFO] Initializing ClipSSD pipeline\")\n",
    "        print(f\"[INFO] Confidence treshold: {confidence_t}\")\n",
    "        print(\"\")\n",
    "\n",
    "    def _propose(self, image_path, original_size):\n",
    "\n",
    "        def _resize_bbox(bbox, in_size, out_size):\n",
    "            \"\"\"\n",
    "            Resize bounding boxes according to image resize.\n",
    "\n",
    "            Args:\n",
    "                bbox: (np.ndarray) bounding boxes of (y_min, x_min, y_max, x_max)\n",
    "                in_size: (tuple) the height and the width of the image before resized\n",
    "                out_size: (tuple) The height and the width of the image after resized\n",
    "            Returns:\n",
    "                (np.ndarray) bounding boxes rescaled according to the given image shapes\n",
    "\n",
    "            \"\"\"\n",
    "            bbox = bbox.copy()\n",
    "            y_scale = float(out_size[0]) / in_size[0]\n",
    "            x_scale = float(out_size[1]) / in_size[1]\n",
    "            bbox[:, 0] = y_scale * bbox[:, 0]\n",
    "            bbox[:, 2] = y_scale * bbox[:, 2]\n",
    "            bbox[:, 1] = x_scale * bbox[:, 1]\n",
    "            bbox[:, 3] = x_scale * bbox[:, 3]\n",
    "            return bbox\n",
    "\n",
    "        bboxes = []\n",
    "\n",
    "        inputs = [self.utils.prepare_input(image_path)]\n",
    "        tensor = self.utils.prepare_tensor(inputs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            detections_batch = self.ssd_model(tensor)\n",
    "\n",
    "        results_per_input = self.utils.decode_results(detections_batch)\n",
    "        best_results_per_input = [self.utils.pick_best(results, self.confidence_t) for results in results_per_input]\n",
    "\n",
    "        bbox, _, _ = best_results_per_input[0]\n",
    "        bbox *= 300\n",
    "        bbox = _resize_bbox(bbox, (300, 300), original_size)\n",
    "        bboxes.append(bbox)\n",
    "\n",
    "        return np.float32(bboxes[0]).tolist()\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=False, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        \"\"\" Pipeline core \"\"\"\n",
    "\n",
    "        # Get sample image\n",
    "        image_path = img_sample.path\n",
    "        img = img_sample.img\n",
    "\n",
    "        # Use SSD to propose relevant objects\n",
    "        bboxes = self._propose(image_path, (img_sample.shape[1], img_sample.shape[2]))\n",
    "\n",
    "        # Handle case where no object is proposed\n",
    "        if len(bboxes) == 0:\n",
    "            return {\"IoU\": 0, \"cosine\": np.nan, \"euclidean\": np.nan, \"dotproduct\": np.nan, \"grounding\": np.nan}\n",
    "\n",
    "        # Use CLIP to encode each relevant object detected\n",
    "        images_encs = list()\n",
    "        for bbox in bboxes:\n",
    "            sub_img = img.crop(bbox)\n",
    "            with torch.no_grad():\n",
    "                sub_img_enc = self._encode_img(sub_img)\n",
    "            images_encs.append(sub_img_enc)\n",
    "        images_encs = torch.cat(images_encs, dim=0)\n",
    "\n",
    "        # Use CLIP to encode the text prompt\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        # Find the best object according to cosine similarity\n",
    "        c_sims = cosine_similarity(prompt_enc, images_encs).squeeze()\n",
    "        best_idx = int(c_sims.argmax())\n",
    "\n",
    "        # Get best bbox\n",
    "        pred_bbox = bboxes[best_idx]\n",
    "\n",
    "        # Use CLIP to encode the prompt\n",
    "        prompt_enc = self._encode_text(prompt).float()\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T  # dot product\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)  # cosine similarity\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()  # euclidean distance\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            display_preds(img, prompt, pred_bbox, gt_bbox, model_name=\"SSD+CLIP\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that said, we can instantiate and use an object for the SSD+CLIP pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate the SSD + CLIP pipeline\n",
    "\n",
    "# SSD + CLIP | with 0.01 confidence\n",
    "ssdnclip = ClipSSD(dataset.categories, confidence_t=0.01, device=device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the SSD + CLIP pipeline\n",
    "\n",
    "ssdnclip(sample, sample.sentences[0], show_ssd=True, show=True, timeit=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 DETR + CLIP\n",
    "\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the DETR + CLIP pipeline\n",
    "\n",
    "# DETR + CLIP\n",
    "detrnclip = DetrClip(dataset.categories, quiet=True, device=device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the DETR + CLIP pipeline\n",
    "\n",
    "detrnclip(sample, sample.sentences[0], show_detr=True, show=True, timeit=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 MDETR (SOTA)\n",
    "\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Class definition for MDETR\n",
    "\n",
    "class MDETRvg(VisualGroundingPipeline):\n",
    "\n",
    "    def __init__(self,\n",
    "                 categories,\n",
    "                 clip_ver=\"RN101\",\n",
    "                 device=\"cpu\",\n",
    "                 quiet=True):\n",
    "\n",
    "        VisualGroundingPipeline.__init__(self, categories, clip_ver, device, quiet)\n",
    "\n",
    "        cpt_url = \"https://pytorch.s3.amazonaws.com/models/multimodal/mdetr/pretrained_resnet101_checkpoint.pth\"\n",
    "\n",
    "        self.MDETR = mdetr_for_phrase_grounding()\n",
    "        self.MDETR.load_state_dict(torch.hub.load_state_dict_from_url(cpt_url)[\"model_ema\"])\n",
    "        self.RoBERTa = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "        self.img_preproc = T.Compose([\n",
    "            T.Resize(800),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        print(\"[INFO] Initializing MDETR pipeline\")\n",
    "        print(\"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_boxes(boxes, size):\n",
    "        w, h = size\n",
    "        b = box_convert(boxes, \"cxcywh\", \"xyxy\")\n",
    "        b = b * torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "        return b\n",
    "\n",
    "    def __call__(self, img_sample, prompt, show=True, timeit=False):\n",
    "\n",
    "        if timeit:\n",
    "            start = time.time()\n",
    "\n",
    "        \"\"\" Pipeline core \"\"\"\n",
    "\n",
    "        # Get sample image\n",
    "        img = Image.open(img_sample.path)\n",
    "\n",
    "        # Make sure image has shape (h, w, 3)\n",
    "        np_image = np.array(img)\n",
    "        if len(np_image.shape) > 3 or (len(np_image.shape) == 3 and np_image.shape[-1] != 3):\n",
    "            np_image = np_image[:, :, 0]\n",
    "        if len(np_image.shape) == 2:\n",
    "            np_image = np.stack((np_image,) * 3, axis=-1)\n",
    "        img = Image.fromarray(np_image)\n",
    "\n",
    "        # Encode the prompt with RoBERTa\n",
    "        enc_text = self.RoBERTa.batch_encode_plus([prompt], padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "        # Preprocess the image for MDETR\n",
    "        img_transformed = self.img_preproc(img)\n",
    "\n",
    "        # Run MDETR on image and prompt\n",
    "        with torch.no_grad():\n",
    "            out = self.MDETR([img_transformed], enc_text[\"input_ids\"]).model_output\n",
    "\n",
    "        # Parse MDETR results to get detections bboxes and probabilities\n",
    "        probs = 1 - out.pred_logits.softmax(-1)[0, :, -1]\n",
    "        boxes_scaled = self.rescale_boxes(out.pred_boxes[0, :], img.size)\n",
    "        mdetr_results = pd.DataFrame(boxes_scaled.squeeze().numpy().reshape(-1, 4))\n",
    "        mdetr_results.columns = [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "        mdetr_results[\"prob\"] = probs.numpy()\n",
    "        mdetr_results = mdetr_results.sort_values(by=['prob'], ascending=False)\n",
    "\n",
    "        # Get best bbox\n",
    "        pred_bbox = mdetr_results.iloc[0, :4].tolist()\n",
    "\n",
    "        # Use CLIP to encode the prompt\n",
    "        prompt_enc = self._encode_text(prompt)\n",
    "\n",
    "        # Crop around the best bbox and encode\n",
    "        pred_image = img.crop(pred_bbox)\n",
    "        pred_image_enc = self._encode_img(pred_image)\n",
    "\n",
    "        # Get ground truth bbox\n",
    "        gt_bbox = img_sample.bbox\n",
    "\n",
    "        \"\"\" Metrics computation \"\"\"\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = self._IoU(pred_bbox, gt_bbox)\n",
    "\n",
    "        # Compute grounding accuracy\n",
    "        grd_correct = self._grounding_accuracy(img_sample, pred_image_enc)\n",
    "\n",
    "        # Compute distance metrics\n",
    "        cosine_sim = cosine_similarity(prompt_enc, pred_image_enc)\n",
    "        euclidean_dist = torch.cdist(prompt_enc, pred_image_enc, p=2).squeeze()\n",
    "        dotproduct = prompt_enc @ pred_image_enc.T\n",
    "\n",
    "        \"\"\" Display results \"\"\"\n",
    "\n",
    "        # Show the final prediction, if requested\n",
    "        if show:\n",
    "            display_preds(img, prompt, pred_bbox, gt_bbox, model_name=\"MDETR\")\n",
    "\n",
    "        # Print execution time, if requested\n",
    "        if timeit:\n",
    "            end = time.time()\n",
    "            print(f\"[INFO] Time elapsed: {end - start:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"IoU\": float(iou),\n",
    "            \"cosine\": float(cosine_sim),\n",
    "            \"euclidean\": float(euclidean_dist),\n",
    "            \"dotproduct\": float(dotproduct),\n",
    "            \"grounding\": float(grd_correct),\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With that said, we can instantiate and use an object for the MDETR pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Instantiate the MDETR pipeline\n",
    "\n",
    "# MDETR for visual grounding\n",
    "mdetr = MDETRvg(dataset.categories, quiet=True, device=device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test the MDETR pipeline\n",
    "\n",
    "mdetr(sample, sample.sentences[0], show=True, timeit=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Testing and results\n",
    "\n",
    "To test each visual grounding framework on the whole test split, a handy function is defined to run the pipeline on each sentence for each sample and compute the average metrics returned. The metrics are also printed in real time to monitor the progress of the testing process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Function to test a given visual grounding pipeline on a given dataset\n",
    "\n",
    "def visual_grounding_test(vg_pipeline, dataset, logging=False):\n",
    "    scores = list()\n",
    "\n",
    "    pbar = tqdm(dataset)\n",
    "\n",
    "    for sample in pbar:\n",
    "\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "\n",
    "            sc = vg_pipeline(sample, sentence, show=False)\n",
    "\n",
    "            scores.append(sc)\n",
    "\n",
    "            avg_metrics = list()\n",
    "\n",
    "            for metric in scores[0].keys():\n",
    "                avg_metric = np.mean([score[metric] for score in scores if score[metric] is not np.nan])\n",
    "                avg_metric = f\"{metric}: {avg_metric:.3f}\"\n",
    "                avg_metrics.append(avg_metric)\n",
    "\n",
    "            pbar_desc = \" | \".join(avg_metrics)\n",
    "\n",
    "            if logging:\n",
    "                pipeline_name = vg_pipeline.__class__.__name__.lower()\n",
    "                datetime_tag = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "                with open(f\"logs/{pipeline_name}_log_{datetime_tag}.txt\", \"a\") as f:\n",
    "                    f.write(\"[\" + datetime_tag + \"] \" + pbar_desc + \"\\n\")\n",
    "\n",
    "            pbar.set_description(pbar_desc)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(You can use the dropdown menu to choose among one of the pipelines define above and test it right now. Still, notice the testing process can be pretty lenghty)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Test one of the pipeline defined\n",
    "\n",
    "pipeline = yolo5clip  #@param [\"yolo5clip\", \"yolo8clip\", \"wshedclip\", \"slicnclip\", \"detrnclip\", \"ssdnclip\", \"mdetr\"]\n",
    "\n",
    "visual_grounding_test(pipeline, test_ds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-04T12:21:07.053671Z",
     "end_time": "2023-06-04T12:21:07.136679Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Results discussion\n",
    "\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Future directions\n",
    "\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Interactive tool\n",
    "\n",
    "Here you can use Colab interactive interface to play with the different implemented pipelines and customize their parameters.\n",
    "\n",
    "#TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
