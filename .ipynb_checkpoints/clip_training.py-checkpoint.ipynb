{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T20:01:12.463499Z",
     "start_time": "2023-04-24T20:01:05.171930Z"
    }
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from refcocog import RefCOCOg, RefCOCOgSample\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")  # CUDA GPU\n",
    "#     print(\"[INFO] GPU found, using GPU.\")\n",
    "# elif torch.has_mps:\n",
    "#     device = torch.device(\"mps\")  # Apple Silicon\n",
    "#     print(\"[INFO] MPS found, using MPS.\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"[INFO] No GPU found, using CPU instead.\")\n",
    "\n",
    "clip_model, prep = clip.load(\"RN50\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T20:01:12.489854Z",
     "start_time": "2023-04-24T20:01:12.465572Z"
    }
   },
   "outputs": [],
   "source": [
    "def training_step(model, dataloader, optimizer, cost_function, device='cuda'):\n",
    "    model.train()\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Performing training step\"):\n",
    "\n",
    "        images, captions = [], []\n",
    "\n",
    "        for sample in batch:\n",
    "            sample = RefCOCOgSample(**sample)\n",
    "\n",
    "            for caption in sample.sentences:\n",
    "                image = Image.open(sample.path)\n",
    "                image = image.crop(sample.bbox)\n",
    "                image = prep(image).to(device)\n",
    "\n",
    "                images.append(image)\n",
    "                captions.append(caption)\n",
    "\n",
    "        captions = clip.tokenize(captions).to(device)\n",
    "        images = torch.stack(images).to(device)\n",
    "\n",
    "        image_logits, text_logits = model(images, captions)\n",
    "\n",
    "        labels = np.arange(images.shape[0])\n",
    "        labels = torch.from_numpy(labels)\n",
    "\n",
    "        loss_i = cost_function(image_logits, labels)\n",
    "        loss_t = cost_function(text_logits, labels)\n",
    "\n",
    "        loss += (loss_i + loss_t) / 2.0\n",
    "\n",
    "    loss = loss.mean()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return loss.item(), 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T20:01:12.513110Z",
     "start_time": "2023-04-24T20:01:12.490551Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_step(model, dataloader, cost_function, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=\"Performing test step\"):\n",
    "\n",
    "            images, captions = [], []\n",
    "\n",
    "            for sample in batch:\n",
    "                sample = RefCOCOgSample(**sample)\n",
    "\n",
    "                for caption in sample.sentences:\n",
    "                    image = Image.open(sample.path)\n",
    "                    image = image.crop(sample.bbox)\n",
    "                    image = prep(image).to(device)\n",
    "\n",
    "                    images.append(image)\n",
    "                    captions.append(caption)\n",
    "\n",
    "            captions = clip.tokenize(captions).to(device)\n",
    "            images = torch.stack(images).to(device)\n",
    "\n",
    "            image_logits, text_logits = model(images, captions)\n",
    "\n",
    "            labels = np.arange(images.shape[0])\n",
    "            labels = torch.from_numpy(labels)\n",
    "\n",
    "            loss_i = cost_function(image_logits, labels)\n",
    "            loss_t = cost_function(text_logits, labels)\n",
    "\n",
    "            loss += (loss_i + loss_t) / 2.0\n",
    "\n",
    "    loss = loss.mean()\n",
    "\n",
    "    return loss.item(), 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T20:01:15.374565Z",
     "start_time": "2023-04-24T20:01:12.512502Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'media/dmmp/vid+backup/Data/refcocog/annotations/refs(umd).p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# dataset = RefCOCOg(ds_path='dataset/refcocog')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mRefCOCOg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedia/dmmp/vid+backup/Data/refcocog\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m      5\u001b[0m train_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.80\u001b[39m\n",
      "File \u001b[0;32m~/DeepLearningLab/visual-grounding/refcocog.py:54\u001b[0m, in \u001b[0;36mRefCOCOg.__init__\u001b[0;34m(self, ds_path, transform)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_path \u001b[38;5;241m=\u001b[39m ds_path\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mds_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/annotations/refs(umd).p\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/annotations/instances.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'media/dmmp/vid+backup/Data/refcocog/annotations/refs(umd).p'"
     ]
    }
   ],
   "source": [
    "# dataset = RefCOCOg(ds_path='dataset/refcocog')\n",
    "dataset = RefCOCOg(ds_path='media/dmmp/vid+backup/Data/refcocog')\n",
    "\n",
    "keep = 0.01\n",
    "train_split = 0.80\n",
    "\n",
    "# keep only a toy portion of the dataset\n",
    "dataset, _ = random_split(dataset, [int(keep * len(dataset)), len(dataset) - int(keep * len(dataset))])\n",
    "\n",
    "train_ds, test_ds = random_split(dataset,\n",
    "                                 [int(train_split * len(dataset)), len(dataset) - int(train_split * len(dataset))])\n",
    "\n",
    "print(f\"Dataset Size: {len(dataset)}\\n---\")\n",
    "print(f\"Train size {len(train_ds)}\")\n",
    "print(f\"Test Size: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing test step:   8%|â–Š         | 4/50 [00:05<01:01,  1.34s/it]"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 3\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "optimizer = torch.optim.Adam(clip_model.parameters(), lr=0.0001, weight_decay=0.000001)\n",
    "\n",
    "cost_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# perform a preliminar step\n",
    "print('Before training:')\n",
    "train_loss, train_accuracy = test_step(clip_model, train_dl, cost_function, device=device)\n",
    "test_loss, test_accuracy = test_step(clip_model, test_dl, cost_function, device=device)\n",
    "\n",
    "print('TRAINING\\n\\tLoss {:.5f}\\n\\tAccuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "print('TEST\\n\\tLoss {:.5f}\\n\\tAccuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "\n",
    "# range over the number of epochs\n",
    "for e in range(epochs):\n",
    "    train_loss, train_accuracy = training_step(clip_model, train_dl, optimizer, cost_function, device=device)\n",
    "    test_loss, test_accuracy = test_step(clip_model, test_dl, cost_function, device=device)\n",
    "    print('Epoch: {:d}'.format(e + 1))\n",
    "    print('TRAINING\\n\\tLoss {:.5f}\\n\\tAccuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('TEST\\n\\tLoss {:.5f}\\n\\tAccuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "\n",
    "# perform final test step and print the final metrics\n",
    "print('After training:')\n",
    "train_loss, train_accuracy = test_step(clip_model, train_dl, cost_function, device=device)\n",
    "test_loss, test_accuracy = test_step(clip_model, test_dl, cost_function, device=device)\n",
    "\n",
    "print('TRAINING\\n\\tLoss {:.5f}\\n\\tAccuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "print('TEST\\n\\tLoss {:.5f}\\n\\tAccuracy {:.2f}'.format(test_loss, test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
