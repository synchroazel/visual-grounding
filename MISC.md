## Useful links

Link to the OpenAI CLIP repo
[https://github.com/openai/CLIP](https://github.com/openai/CLIP)

## Literature

- "A Review of Visual Grounding Tasks in Natural Language Processing" by Raffaella Bernardi et al. (2020) - This article
  provides an overview of the different visual grounding tasks that have been studied in the context of natural language
  processing (NLP), including image captioning, visual question answering, and referring expression comprehension.
- "Neural Network Models for Visual Grounding: A Survey" by Rohan Chandra et al. (2021) - This survey paper provides an
  in-depth look at various neural network models that have been used for visual grounding tasks, including convolutional
  neural networks (CNNs), recurrent neural networks (RNNs), and transformers.
- "A Dataset of Spatial Descriptions for Visual Grounding" by Dima Damen et al. (2018) - This paper introduces a dataset
  of spatial descriptions for visual grounding, which can be used to train and evaluate models for tasks such as
  referring expression comprehension.
- "Learning Visual Attributes for Visual Grounding" by Lisa Anne Hendricks et al. (2018) - This paper proposes a model
  for learning visual attributes that can be used for visual grounding tasks such as referring expression comprehension
  and image retrieval.
- "Language Grounding in Deep Neural Networks" by Kyunghyun Cho et al. (2021) - This review paper discusses various
  approaches to language grounding in deep neural networks, including visual grounding and semantic grounding, and
  provides an overview of the challenges and future directions in this area.

References by the assignment paper:

- Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance seg- mentation. In
  Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8759–8768, 2018.
- Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
  Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In
  International conference on machine learning, pages 8748–8763. PMLR, 2021.
- Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring
  expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
  Proceedings, Part II 14, pages 69–85. Springer, 2016.