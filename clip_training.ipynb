{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CLIP\n",
    "\n",
    "The following notebook shows two different takes at fine-tuning CLIP.\n",
    "\n",
    "1. Fine tuning CLIP visual backbone on the RefCOCOg dataset (images)\n",
    "2. Fine tuning CLIP performing contrastive learning on the RefCOCOg dataset (text + images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:26.528476Z",
     "start_time": "2023-04-29T14:10:22.134777Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 17:35:39.960056: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-02 17:35:39.980293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-02 17:35:40.325477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GPU found, using GPU.\n"
     ]
    }
   ],
   "source": [
    "#@title Import necessary packages and set correct device\n",
    "\n",
    "import os\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.refcocog import RefCOCOg, RefCOCOgSample\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"[INFO] GPU found, using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"[INFO] No GPU found, using CPU instead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZERS_TO_TRY = {\n",
    "    \"SGD\":torch.optim.SGD,\n",
    "    \"RMSProp\":torch.optim.RMSprop,\n",
    "    \"Adam\":torch.optim.Adam,\n",
    "    \"Adamax\":torch.optim.Adamax,\n",
    "    \"Adadelta\":torch.optim.Adadelta,\n",
    "    # todo: add more\n",
    "}\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "\n",
    "batch_size = 128 # 256 causes out of memory with 24GB of GPU ram\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "optimizer = \"Adam\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:29.919895Z",
     "start_time": "2023-04-29T14:10:26.461935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model params: 102,007,137\n",
      "[INFO] Trainable params: 102,007,137\n",
      "[INFO] Input resolution:  224\n",
      "[INFO] Max prompt length: 77\n",
      "[INFO] Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "#@title Import CLIP model and show its info\n",
    "\n",
    "clip_model, clip_prep = clip.load(\"RN50\", device=device)\n",
    "\n",
    "print(\"[INFO] Model params: {:,}\".format(np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()])))\n",
    "print(\"[INFO] Trainable params: {:,}\".format(sum(p.numel() for p in clip_model.parameters() if p.requires_grad)))\n",
    "print(\"[INFO] Input resolution: \", clip_model.visual.input_resolution)\n",
    "print(\"[INFO] Max prompt length:\", clip_model.context_length)\n",
    "print(\"[INFO] Vocab size:\", clip_model.vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, solely for debuggining purposes on local machines, we are discarding most of the dataset and using only a \"toy\" portion of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:53.914478Z",
     "start_time": "2023-04-29T14:10:29.736385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 4982\n",
      "\n",
      "Train size: 4222\n",
      "Val size:   257\n",
      "Test size:  502\n"
     ]
    }
   ],
   "source": [
    "#@title Import RefCOCOg dataset and its train/val/test splits\n",
    "\n",
    "# modify\n",
    "dataset_path = \"/media/dmmp/vid+backup/Data/refcocog\"\n",
    "\n",
    "dataset = RefCOCOg(ds_path=dataset_path)\n",
    "\n",
    "train_ds = RefCOCOg(ds_path=dataset_path, split='train')\n",
    "val_ds = RefCOCOg(ds_path=dataset_path, split='val')\n",
    "test_ds = RefCOCOg(ds_path=dataset_path, split='test')\n",
    "\n",
    "keep = 0.1\n",
    "dataset, _ = random_split(dataset, [int(keep * len(dataset)), len(dataset) - int(keep * len(dataset))])\n",
    "train_ds, _ = random_split(train_ds, [int(keep * len(train_ds)), len(train_ds) - int(keep * len(train_ds))])\n",
    "val_ds, _ = random_split(val_ds, [int(keep * len(val_ds)), len(val_ds) - int(keep * len(val_ds))])\n",
    "test_ds, _ = random_split(test_ds, [int(keep * len(test_ds)), len(test_ds) - int(keep * len(test_ds))])\n",
    "\n",
    "print(f\"Dataset Size: {len(dataset)}\\n\")\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Val size:   {len(val_ds)}\")\n",
    "print(f\"Test size:  {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.042399Z",
     "start_time": "2023-04-29T14:10:53.917978Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title An example of computing images-prompts similarity\n",
    "\n",
    "### Useless for the actual training loop, but was in Alessandro's code.\n",
    "### But functions below might be helpful later ¯\\(ツ)/¯\n",
    "\n",
    "def get_data(dataset):\n",
    "    texts, images = list(), list()\n",
    "\n",
    "    for sample in tqdm(dataset, desc=\"[INFO] Loading images and captions\"):\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "            images.append(sample.path)\n",
    "            texts.append(sentence)\n",
    "\n",
    "    return images, texts\n",
    "\n",
    "\n",
    "def encode_data(images_fp: list[str], texts: list[str]):\n",
    "    # preprocess the images to transform from filenames to images to tensors\n",
    "    images = [clip_prep(Image.open(image)) for image in tqdm(images_fp, desc=\"[INFO] Preprocessing images\")]\n",
    "    images = torch.tensor(np.stack(images)).to(device)\n",
    "\n",
    "    # preprocess the texts to transform from text to tensors\n",
    "    text_tokens = clip.tokenize([\"This is \" + desc for desc in tqdm(texts, desc=\"[INFO] Preprocessing texts\")]).to(\n",
    "        device)\n",
    "\n",
    "    # encode the inputs\n",
    "    with torch.no_grad():\n",
    "        print(\"[INFO] Encoding images...\")\n",
    "        images_z = clip_model.encode_image(images).float()\n",
    "        print(\"[INFO] Encoding texts...\")\n",
    "        texts_z = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "    return images_z, texts_z\n",
    "\n",
    "\n",
    "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
    "    # normalise the image and the text\n",
    "    images_z /= images_z.norm(dim=-1, keepdim=True)\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # evaluate the cosine similarity between the sets of features\n",
    "    similarity = (texts_z @ images_z.T)\n",
    "\n",
    "    return similarity.cpu()\n",
    "\n",
    "# images_fp, texts = get_data(test_ds)\n",
    "\n",
    "# images_z, texts_z = encode_data(images_fp, texts)\n",
    "\n",
    "# similarity = cosine_similarity(images_z, texts_z)\n",
    "\n",
    "# print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune by classifying on RefCOCOg classes\n",
    "\n",
    "In the following approach we're gonna:\n",
    "- create a custom CLIP architecture with an additional trainable layer\n",
    "- implement training and testing logics\n",
    "- train the newly created CLIP model on the images from the whole dataset\n",
    "\n",
    "This should bring some **benefits**:\n",
    "- CLIP should become *better* at extracting features from our RefCOCOg images\n",
    "- [add others, if any]\n",
    "\n",
    "and some **drawbacks** too:\n",
    "- The textual encoding training is basically lost\n",
    "- [add others, if any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.527738Z",
     "start_time": "2023-04-29T14:10:53.978706Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Custom CLIP architecture featuring an additional fc layer\n",
    "\n",
    "class CustomCLIP(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        model, _ = clip.load(\"RN50\")\n",
    "\n",
    "        # take the visual encoder of CLIP\n",
    "        # we also convert it to be 32 bit (by default CLIP is 16)\n",
    "        self.encoder = model.visual.float()\n",
    "\n",
    "        # add a linear layer\n",
    "        self.classifier = torch.nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.544572Z",
     "start_time": "2023-04-29T14:10:54.029665Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Training and test logics\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum, optimizer):\n",
    "    try:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.classifier.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd, momentum=momentum)\n",
    "    except TypeError:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.classifier.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_cost_function():\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "    return cost_function\n",
    "\n",
    "\n",
    "def training_step(net, data_loader, optimizer, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    # iterate over the training set\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Training step\")):\n",
    "\n",
    "        inputs, targets = list(), list()\n",
    "\n",
    "        for sample in batch:\n",
    "            sample = RefCOCOgSample(**sample)\n",
    "\n",
    "            prep_img = clip_prep(sample.img)\n",
    "\n",
    "            inputs.append(prep_img)\n",
    "            targets.append(sample.category_id - 1)  # so that category_ids will start from #0\n",
    "\n",
    "        inputs = torch.stack(inputs)\n",
    "        targets = torch.tensor(targets)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fetch prediction and loss value\n",
    "        n_samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(dim=1)  # max() returns (maximum_value, index_of_maximum_value)\n",
    "\n",
    "        # compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / n_samples, cumulative_accuracy / n_samples * 100\n",
    "\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=device):\n",
    "    samples_ = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    with torch.no_grad():\n",
    "        # iterate over the test set\n",
    "        for batch_idx, samples in enumerate(tqdm(data_loader, desc=\"[INFO] Test step\")):\n",
    "\n",
    "            inputs, targets = list(), list()\n",
    "\n",
    "            for sample in samples:\n",
    "                sample = RefCOCOgSample(**sample)\n",
    "\n",
    "                prep_img = clip_prep(sample.img)\n",
    "\n",
    "                inputs.append(prep_img)\n",
    "                targets.append(sample.category_id - 1)  # so that category_ids will start from #0\n",
    "\n",
    "            inputs = torch.stack(inputs)\n",
    "            targets = torch.tensor(targets)\n",
    "\n",
    "            # load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # fetch prediction and loss value\n",
    "            samples_ += inputs.shape[0]\n",
    "            cumulative_loss += loss.item()  # Note: the .item() is needed to extract scalars from tensors\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples_, cumulative_accuracy / samples_ * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.622225Z",
     "start_time": "2023-04-29T14:10:54.122760Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Main training loop logic\n",
    "\n",
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "\n",
    "def training_loop(train_ds,\n",
    "         val_ds,\n",
    "         test_ds,\n",
    "         batch_size=batch_size,\n",
    "         num_classes=90,  # 90 classes in RefCOCOg\n",
    "         device=device,\n",
    "         learning_rate=learning_rate,\n",
    "         weight_decay=0.000001,\n",
    "         momentum=momentum,\n",
    "         epochs=epochs,\n",
    "         optimizer = optimizer):\n",
    "    # create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "    # instantiate the network and move it to the chosen device (GPU)\n",
    "    net = CustomCLIP(num_classes=num_classes).to(device)\n",
    "\n",
    "    # instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum, optimizer)\n",
    "\n",
    "    # define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    # computes evaluation results before training\n",
    "    print('Before training:')\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # for each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        # logs to TensorBoard\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "        print('Epoch: {:d}'.format(e + 1))\n",
    "        print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "        print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "    # compute final evaluation results\n",
    "    print('After training:')\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # closes the logger\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.623506Z",
     "start_time": "2023-04-29T14:10:54.174153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Test step: 100%|█████████████████████████| 33/33 [01:30<00:00,  2.75s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:05<00:00,  1.75s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 4/4 [00:12<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss 0.03524, Training accuracy 0.26\n",
      "\tValidation loss 0.05263, Validation accuracy 0.00\n",
      "\tTest loss 0.03595, Test accuracy 0.00\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:08<00:00,  2.06s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tTraining loss 0.03371, Training accuracy 31.43\n",
      "\tValidation loss 0.04624, Validation accuracy 52.53\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:07<00:00,  2.06s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "\tTraining loss 0.03058, Training accuracy 57.01\n",
      "\tValidation loss 0.04414, Validation accuracy 59.53\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:08<00:00,  2.07s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "\tTraining loss 0.02777, Training accuracy 59.76\n",
      "\tValidation loss 0.04130, Validation accuracy 60.70\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:08<00:00,  2.07s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "\tTraining loss 0.02525, Training accuracy 59.92\n",
      "\tValidation loss 0.03720, Validation accuracy 61.48\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:08<00:00,  2.09s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "\tTraining loss 0.02302, Training accuracy 59.97\n",
      "\tValidation loss 0.03294, Validation accuracy 61.48\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:08<00:00,  2.07s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "\tTraining loss 0.02105, Training accuracy 59.97\n",
      "\tValidation loss 0.03473, Validation accuracy 62.26\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:08<00:00,  2.08s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "\tTraining loss 0.01937, Training accuracy 60.30\n",
      "\tValidation loss 0.02937, Validation accuracy 61.48\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:08<00:00,  2.07s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "\tTraining loss 0.01789, Training accuracy 60.37\n",
      "\tValidation loss 0.02880, Validation accuracy 62.65\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:08<00:00,  2.06s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "\tTraining loss 0.01666, Training accuracy 60.73\n",
      "\tValidation loss 0.02520, Validation accuracy 62.65\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step: 100%|█████████████████████| 33/33 [01:07<00:00,  2.06s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "\tTraining loss 0.01559, Training accuracy 61.51\n",
      "\tValidation loss 0.01726, Validation accuracy 62.65\n",
      "-----------------------------------------------------\n",
      "After training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Test step: 100%|█████████████████████████| 33/33 [01:03<00:00,  1.91s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:03<00:00,  1.29s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 4/4 [00:07<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss 0.01489, Training accuracy 62.20\n",
      "\tValidation loss 0.03044, Validation accuracy 62.65\n",
      "\tTest loss 0.01611, Test accuracy 61.35\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Execute main training loop\n",
    "\n",
    "training_loop(train_ds, val_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Fine-tune by contrastive learning on objects+texts\n",
    "\n",
    "In this other approach we're gonna:\n",
    "- import the original CLIP, without adding other layers on top\n",
    "- implement the contrastive loss logic and adapt it into the training and test step defined earlier\n",
    "- train the model on all images - caption pairs in the dataset, using the contrastive loss\n",
    "\n",
    "Note that when speaking of images, we do not mean the whole sample images, but the images cropped at the ground truth bbox. In other words, we are training CLIP to maximize the similarity between the embedding of each image of every object refered by the dataset and the corresponding (1 or more) caption/s.\n",
    "\n",
    "This should bring some **benefits**:\n",
    "- CLIP should become *better* at extracting embeddings for the images and captions of RefCOCOg.\n",
    "- [add others, if any]\n",
    "\n",
    "and some **drawbacks** too:\n",
    "- as before, CLIP zero-shot capabilities would be basically lost\n",
    "- [add others, if any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.019106Z",
     "start_time": "2023-04-29T14:10:54.247540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model params: 102,007,137\n",
      "[INFO] Trainable params: 102,007,137\n",
      "[INFO] Input resolution:  224\n",
      "[INFO] Max prompt length: 77\n",
      "[INFO] Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "#@title Import CLIP model and show its info\n",
    "\n",
    "clip_model, clip_prep = clip.load(\"RN50\", device=device)\n",
    "\n",
    "print(\"[INFO] Model params: {:,}\".format(np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()])))\n",
    "print(\"[INFO] Trainable params: {:,}\".format(sum(p.numel() for p in clip_model.parameters() if p.requires_grad)))\n",
    "print(\"[INFO] Input resolution: \", clip_model.visual.input_resolution)\n",
    "print(\"[INFO] Max prompt length:\", clip_model.context_length)\n",
    "print(\"[INFO] Vocab size:\", clip_model.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.111368Z",
     "start_time": "2023-04-29T14:10:59.028213Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Contrastive loss function definition\n",
    "\n",
    "def contrastive_loss(image_logits, text_logits, cost_function):\n",
    "    labels = np.arange(image_logits.shape[0])\n",
    "    labels = torch.from_numpy(labels).to(device)\n",
    "\n",
    "    loss_i = cost_function(image_logits, labels)\n",
    "    loss_t = cost_function(text_logits, labels)\n",
    "\n",
    "    return (loss_i + loss_t) / 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.203471Z",
     "start_time": "2023-04-29T14:10:59.122680Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Slight modifications to the aforementioned logics\n",
    "\n",
    "\n",
    "def training_step_cl(net, data_loader, optimizer, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "\n",
    "    # set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Training step\")):\n",
    "\n",
    "        images, texts = list(), list()\n",
    "\n",
    "        for sample in batch:\n",
    "            sample = RefCOCOgSample(**sample)\n",
    "\n",
    "            for sentence in sample.sentences:\n",
    "                prep_img = sample.img.crop(sample.bbox)\n",
    "                prep_img = clip_prep(prep_img)\n",
    "\n",
    "                images.append(prep_img)\n",
    "                texts.append(sentence)\n",
    "\n",
    "        texts = clip.tokenize(texts).to(device)\n",
    "        images = torch.stack(images).to(device)\n",
    "\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        image_logits, text_logits = net(images, texts)\n",
    "\n",
    "        # loss computation\n",
    "        loss = contrastive_loss(image_logits, text_logits, cost_function)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fetch loss value\n",
    "        n_samples += images.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss / n_samples\n",
    "\n",
    "\n",
    "def test_step_cl(net, data_loader, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Test step\")):\n",
    "\n",
    "            images, texts = list(), list()\n",
    "\n",
    "            for sample in batch:\n",
    "                sample = RefCOCOgSample(**sample)\n",
    "\n",
    "                for sentence in sample.sentences:\n",
    "                    prep_img = sample.img.crop(sample.bbox)\n",
    "                    prep_img = clip_prep(prep_img)\n",
    "\n",
    "                    images.append(prep_img)\n",
    "                    texts.append(sentence)\n",
    "\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "            images = torch.stack(images).to(device)\n",
    "\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            image_logits, text_logits = net(images, texts)\n",
    "\n",
    "            # loss computation\n",
    "            loss = contrastive_loss(image_logits, text_logits, cost_function)\n",
    "\n",
    "            # fetch loss value\n",
    "            n_samples += images.shape[0]\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss / n_samples\n",
    "\n",
    "\n",
    "def log_values_cl(writer, step, loss, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "\n",
    "\n",
    "def main_loop_cl(train_ds,\n",
    "                 val_ds,\n",
    "                 test_ds,\n",
    "                 batch_size=batch_size,\n",
    "                 num_classes=90,  # 90 classes in RefCOCOg\n",
    "                 device=device,\n",
    "                 learning_rate=learning_rate,\n",
    "                 weight_decay=0.000001,\n",
    "                 momentum=momentum,\n",
    "                 epochs=epochs,\n",
    "                 optimizer=optimizer):\n",
    "    # create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "    # instantiate the network and move it to the chosen device (GPU)\n",
    "    net = CustomCLIP(num_classes=num_classes).to(device)\n",
    "\n",
    "    # instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum, optimizer)\n",
    "\n",
    "    # define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    # computes evaluation results before training\n",
    "    # tODO: fix error here: CustomCLIP.forward() takes 2 positional arguments but 3 were given\n",
    "    print('Before training:')\n",
    "    train_loss = test_step_cl(net, train_loader, cost_function)\n",
    "    val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "    test_loss = test_step_cl(net, test_loader, cost_function)\n",
    "    \n",
    "    # print(train_loss)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values_cl(writer, -1, train_loss, \"train\")\n",
    "    log_values_cl(writer, -1, val_loss, \"validation\")\n",
    "    log_values_cl(writer, -1, test_loss, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "    print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "    print('\\tTest loss {:.5f}'.format(test_loss))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # for each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss = training_step_cl(net, train_loader, optimizer, cost_function)\n",
    "        val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "\n",
    "        # logs to TensorBoard\n",
    "        log_values_cl(writer, e, val_loss, \"Validation\")\n",
    "\n",
    "        print('Epoch: {:d}'.format(e + 1))\n",
    "        print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "        print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "    # compute final evaluation results\n",
    "    print('After training:')\n",
    "    train_loss = test_step_cl(net, train_loader, cost_function)\n",
    "    val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "    test_loss = test_step_cl(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values_cl(writer, epochs, train_loss, \"train\")\n",
    "    log_values_cl(writer, epochs, val_loss, \"validation\")\n",
    "    log_values_cl(writer, epochs, test_loss, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "    print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "    print('\\tTest loss {:.5f}'.format(test_loss))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # closes the logger\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.339097Z",
     "start_time": "2023-04-29T14:10:59.217477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Test step:   0%|                                  | 0/33 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CustomCLIP.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Execute main training loop\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmain_loop_cl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mmain_loop_cl\u001b[0;34m(train_ds, val_ds, test_ds, batch_size, num_classes, device, learning_rate, weight_decay, momentum, epochs, optimizer)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# computes evaluation results before training\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore training:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest_step_cl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m test_step_cl(net, val_loader, cost_function)\n\u001b[1;32m    130\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m test_step_cl(net, test_loader, cost_function)\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mtest_step_cl\u001b[0;34m(net, data_loader, cost_function, device)\u001b[0m\n\u001b[1;32m     80\u001b[0m texts \u001b[38;5;241m=\u001b[39m texts\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m image_logits, text_logits \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# loss computation\u001b[39;00m\n\u001b[1;32m     86\u001b[0m loss \u001b[38;5;241m=\u001b[39m contrastive_loss(image_logits, text_logits, cost_function)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: CustomCLIP.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#@title Execute main training loop\n",
    "\n",
    "main_loop_cl(train_ds, val_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Grounding test\n",
    "\n",
    "The following is to verify the effect of these approaches to the main visual grounding task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.474492Z",
     "start_time": "2023-04-29T14:10:59.373451Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Function definition to test visual grounding with a given pipeline\n",
    "\n",
    "def visual_grounding_test(vg_pipeline, dataset):\n",
    "    scores = list()\n",
    "\n",
    "    for sample in tqdm(dataset, desc=f\"Testing on {len(dataset)} images\"):\n",
    "\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "\n",
    "            try:\n",
    "                sc = vg_pipeline(sample, sentence, show=False)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            scores.append(sc)\n",
    "\n",
    "    for metric in scores[0].keys():\n",
    "        avg_metric = np.mean([score[metric] for score in scores])\n",
    "\n",
    "        print(\"Avg. {}: {:.3f}\".format(metric, avg_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T08:51:26.185676Z",
     "start_time": "2023-04-27T08:37:24.375443Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmmp/.local/lib/python3.10/site-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /home/dmmp/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/dmmp/.cache/torch/hub/requirements.txt not found, check failed.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid CUDA '--device cuda' requested, use '--device cpu' or pass valid CUDA device(s). Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:46\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained \u001b[38;5;129;01mand\u001b[39;00m channels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m80\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/utils/torch_utils.py:118\u001b[0m, in \u001b[0;36mselect_device\u001b[0;34m(device, batch_size, newline)\u001b[0m\n\u001b[1;32m    117\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m device  \u001b[38;5;66;03m# set environment variable - must be before assert is_available()\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(device\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)), \\\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requested, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--device cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or pass valid CUDA device(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mps \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# prefer GPU if available\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Invalid CUDA '--device cuda' requested, use '--device cpu' or pass valid CUDA device(s)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Import the YoloClip pipeline and test it on the test dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myoloclip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YoloClip\n\u001b[0;32m----> 5\u001b[0m yoloclip \u001b[38;5;241m=\u001b[39m \u001b[43mYoloClip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m visual_grounding_test(yoloclip, test_ds)\n",
      "File \u001b[0;32m~/DeepLearningLab/visual-grounding/modules/yoloclip.py:60\u001b[0m, in \u001b[0;36mYoloClip.__init__\u001b[0;34m(self, categories, yolo_ver, clip_ver, device, quiet, dist_metric)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m              categories,\n\u001b[1;32m     54\u001b[0m              yolo_ver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov5s\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m              quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m              dist_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics/yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_prep \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(clip_ver, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquiet \u001b[38;5;241m=\u001b[39m quiet\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/hub.py:558\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    556\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 558\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/hub.py:587\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[1;32m    586\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m--> 587\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:93\u001b[0m, in \u001b[0;36myolov5s\u001b[0;34m(pretrained, channels, classes, autoshape, _verbose, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myolov5s\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# YOLOv5-small model https://github.com/ultralytics/yolov5\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:78\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m help_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     77\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for help.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Invalid CUDA '--device cuda' requested, use '--device cpu' or pass valid CUDA device(s). Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
     ]
    }
   ],
   "source": [
    "#@title Import the YoloClip pipeline and test it on the test dataset\n",
    "\n",
    "from modules.yoloclip import YoloClip\n",
    "\n",
    "yoloclip = YoloClip(device=device, quiet=True, categories=dataset.dataset.categories)\n",
    "\n",
    "visual_grounding_test(yoloclip, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
