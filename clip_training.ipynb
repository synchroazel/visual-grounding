{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CLIP\n",
    "\n",
    "The following notebook shows two different takes at fine-tuning CLIP.\n",
    "\n",
    "1. Fine tuning CLIP visual backbone on the RefCOCOg dataset (images)\n",
    "2. Fine tuning CLIP performing contrastive learning on the RefCOCOg dataset (text + images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:26.528476Z",
     "start_time": "2023-04-29T14:10:22.134777Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import necessary packages and set correct device\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.refcocog import RefCOCOg, RefCOCOgSample\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # CUDA GPU\n",
    "    print(\"[INFO] Using GPU.\")\n",
    "elif torch.has_mps:\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "    print(\"[INFO] Using MPS.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"[INFO] No GPU found, using CPU instead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZERS_TO_TRY = {\n",
    "    \"SGD\": torch.optim.SGD,\n",
    "    \"RMSProp\": torch.optim.RMSprop,\n",
    "    \"Adam\": torch.optim.Adam,\n",
    "    \"Adamax\": torch.optim.Adamax,\n",
    "    \"Adadelta\": torch.optim.Adadelta,\n",
    "    # TODO: add more\n",
    "}\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "\n",
    "batch_size = 128  # 256 causes out of memory with 24GB of GPU ram\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "optimizer = \"Adam\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:29.919895Z",
     "start_time": "2023-04-29T14:10:26.461935Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import CLIP model and show its info\n",
    "\n",
    "clip_model, clip_prep = clip.load(\"RN50\", device=device)\n",
    "\n",
    "print(\"[INFO] Model params: {:,}\".format(np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()])))\n",
    "print(\"[INFO] Trainable params: {:,}\".format(sum(p.numel() for p in clip_model.parameters() if p.requires_grad)))\n",
    "print(\"[INFO] Input resolution: \", clip_model.visual.input_resolution)\n",
    "print(\"[INFO] Max prompt length:\", clip_model.context_length)\n",
    "print(\"[INFO] Vocab size:\", clip_model.vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, solely for debuggining purposes on local machines, we are discarding most of the dataset and using only a \"toy\" portion of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Import RefCOCOg dataset and its train/val/test splits\n",
    "\n",
    "# data_path = \"/media/dmmp/vid+backup/Data/refcocog\"\n",
    "data_path = \"dataset/refcocog\"\n",
    "\n",
    "dataset = RefCOCOg(ds_path=data_path)\n",
    "\n",
    "train_ds = RefCOCOg(ds_path=data_path, split='train')\n",
    "val_ds = RefCOCOg(ds_path=data_path, split='val')\n",
    "test_ds = RefCOCOg(ds_path=data_path, split='test')\n",
    "\n",
    "# keep only a toy portion of each split\n",
    "keep = 0.1\n",
    "dataset, _ = random_split(dataset, [int(keep * len(dataset)), len(dataset) - int(keep * len(dataset))])\n",
    "train_ds, _ = random_split(train_ds, [int(keep * len(train_ds)), len(train_ds) - int(keep * len(train_ds))])\n",
    "val_ds, _ = random_split(val_ds, [int(keep * len(val_ds)), len(val_ds) - int(keep * len(val_ds))])\n",
    "test_ds, _ = random_split(test_ds, [int(keep * len(test_ds)), len(test_ds) - int(keep * len(test_ds))])\n",
    "\n",
    "print(f\"Dataset Size: {len(dataset)}\\n\")\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Val size:   {len(val_ds)}\")\n",
    "print(f\"Test size:  {len(test_ds)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:53.914478Z",
     "start_time": "2023-04-29T14:10:29.736385Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import RefCOCOg dataset and its train/val/test splits\n",
    "\n",
    "# modify\n",
    "dataset_path = \"/media/dmmp/vid+backup/Data/refcocog\"\n",
    "\n",
    "dataset = RefCOCOg(ds_path=dataset_path)\n",
    "\n",
    "train_ds = RefCOCOg(ds_path=dataset_path, split='train')\n",
    "val_ds = RefCOCOg(ds_path=dataset_path, split='val')\n",
    "test_ds = RefCOCOg(ds_path=dataset_path, split='test')\n",
    "\n",
    "keep = 0.1\n",
    "dataset, _ = random_split(dataset, [int(keep * len(dataset)), len(dataset) - int(keep * len(dataset))])\n",
    "train_ds, _ = random_split(train_ds, [int(keep * len(train_ds)), len(train_ds) - int(keep * len(train_ds))])\n",
    "val_ds, _ = random_split(val_ds, [int(keep * len(val_ds)), len(val_ds) - int(keep * len(val_ds))])\n",
    "test_ds, _ = random_split(test_ds, [int(keep * len(test_ds)), len(test_ds) - int(keep * len(test_ds))])\n",
    "\n",
    "print(f\"Dataset Size: {len(dataset)}\\n\")\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Val size:   {len(val_ds)}\")\n",
    "print(f\"Test size:  {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.042399Z",
     "start_time": "2023-04-29T14:10:53.917978Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title An example of computing images-prompts similarity\n",
    "\n",
    "### Useless for the actual training loop, but was in Alessandro's code.\n",
    "### But functions below might be helpful later ¯\\(ツ)/¯\n",
    "\n",
    "def get_data(dataset):\n",
    "    texts, images = list(), list()\n",
    "\n",
    "    for sample in tqdm(dataset, desc=\"[INFO] Loading images and captions\"):\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "            images.append(sample.path)\n",
    "            texts.append(sentence)\n",
    "\n",
    "    return images, texts\n",
    "\n",
    "\n",
    "def encode_data(images_fp: list[str], texts: list[str]):\n",
    "    # preprocess the images to transform from filenames to images to tensors\n",
    "    images = [clip_prep(Image.open(image)) for image in tqdm(images_fp, desc=\"[INFO] Preprocessing images\")]\n",
    "    images = torch.tensor(np.stack(images)).to(device)\n",
    "\n",
    "    # preprocess the texts to transform from text to tensors\n",
    "    text_tokens = clip.tokenize([\"This is \" + desc for desc in tqdm(texts, desc=\"[INFO] Preprocessing texts\")]).to(\n",
    "        device)\n",
    "\n",
    "    # encode the inputs\n",
    "    with torch.no_grad():\n",
    "        print(\"[INFO] Encoding images...\")\n",
    "        images_z = clip_model.encode_image(images).float()\n",
    "        print(\"[INFO] Encoding texts...\")\n",
    "        texts_z = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "    return images_z, texts_z\n",
    "\n",
    "\n",
    "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
    "    # normalise the image and the text\n",
    "    images_z /= images_z.norm(dim=-1, keepdim=True)\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # evaluate the cosine similarity between the sets of features\n",
    "    similarity = (texts_z @ images_z.T)\n",
    "\n",
    "    return similarity.cpu()\n",
    "\n",
    "# images_fp, texts = get_data(test_ds)\n",
    "\n",
    "# images_z, texts_z = encode_data(images_fp, texts)\n",
    "\n",
    "# similarity = cosine_similarity(images_z, texts_z)\n",
    "\n",
    "# print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune by classifying on RefCOCOg classes\n",
    "\n",
    "In the following approach we're gonna:\n",
    "- create a custom CLIP architecture with an additional trainable layer\n",
    "- implement training and testing logics\n",
    "- train the newly created CLIP model on the images from the whole dataset\n",
    "\n",
    "This should bring some **benefits**:\n",
    "- CLIP should become *better* at extracting features from our RefCOCOg images\n",
    "- [add others, if any]\n",
    "\n",
    "and some **drawbacks** too:\n",
    "- The textual encoding training is basically lost\n",
    "- [add others, if any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.527738Z",
     "start_time": "2023-04-29T14:10:53.978706Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Custom CLIP architecture featuring an additional fc layer\n",
    "\n",
    "class CustomCLIP(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        model, _ = clip.load(\"RN50\")\n",
    "\n",
    "        # take the visual encoder of CLIP\n",
    "        # we also convert it to be 32 bit (by default CLIP is 16)\n",
    "        self.encoder = model.visual.float()\n",
    "\n",
    "        # add a linear layer\n",
    "        self.classifier = torch.nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.544572Z",
     "start_time": "2023-04-29T14:10:54.029665Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Training and test logics\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum, optimizer):\n",
    "    try:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.classifier.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd, momentum=momentum)\n",
    "    except TypeError:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.classifier.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_cost_function():\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "    return cost_function\n",
    "\n",
    "\n",
    "def training_step(net, data_loader, optimizer, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    # iterate over the training set\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Training step\")):\n",
    "\n",
    "        inputs, targets = list(), list()\n",
    "\n",
    "        for sample in batch:\n",
    "            sample = RefCOCOgSample(**sample)\n",
    "\n",
    "            prep_img = clip_prep(sample.img)\n",
    "\n",
    "            inputs.append(prep_img)\n",
    "            targets.append(sample.category_id - 1)  # so that category_ids will start from #0\n",
    "\n",
    "        inputs = torch.stack(inputs)\n",
    "        targets = torch.tensor(targets)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fetch prediction and loss value\n",
    "        n_samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(dim=1)  # max() returns (maximum_value, index_of_maximum_value)\n",
    "\n",
    "        # compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / n_samples, cumulative_accuracy / n_samples * 100\n",
    "\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=device):\n",
    "    samples_ = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    with torch.no_grad():\n",
    "        # iterate over the test set\n",
    "        for batch_idx, samples in enumerate(tqdm(data_loader, desc=\"[INFO] Test step\")):\n",
    "\n",
    "            inputs, targets = list(), list()\n",
    "\n",
    "            for sample in samples:\n",
    "                sample = RefCOCOgSample(**sample)\n",
    "\n",
    "                prep_img = clip_prep(sample.img)\n",
    "\n",
    "                inputs.append(prep_img)\n",
    "                targets.append(sample.category_id - 1)  # so that category_ids will start from #0\n",
    "\n",
    "            inputs = torch.stack(inputs)\n",
    "            targets = torch.tensor(targets)\n",
    "\n",
    "            # load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # fetch prediction and loss value\n",
    "            samples_ += inputs.shape[0]\n",
    "            cumulative_loss += loss.item()  # Note: the .item() is needed to extract scalars from tensors\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples_, cumulative_accuracy / samples_ * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.622225Z",
     "start_time": "2023-04-29T14:10:54.122760Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Main training loop logic\n",
    "\n",
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "\n",
    "def training_loop(train_ds,\n",
    "                  val_ds,\n",
    "                  test_ds,\n",
    "                  batch_size=batch_size,\n",
    "                  num_classes=90,  # 90 classes in RefCOCOg\n",
    "                  device=device,\n",
    "                  learning_rate=learning_rate,\n",
    "                  weight_decay=0.000001,\n",
    "                  momentum=momentum,\n",
    "                  epochs=epochs,\n",
    "                  optimizer=optimizer):\n",
    "    # create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "    # instantiate the network and move it to the chosen device (GPU)\n",
    "    net = CustomCLIP(num_classes=num_classes).to(device)\n",
    "\n",
    "    # instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum, optimizer)\n",
    "\n",
    "    # define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    # computes evaluation results before training\n",
    "    print('Before training:')\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # for each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        # logs to TensorBoard\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "        print('Epoch: {:d}'.format(e + 1))\n",
    "        print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "        print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "    # compute final evaluation results\n",
    "    print('After training:')\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # closes the logger\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.623506Z",
     "start_time": "2023-04-29T14:10:54.174153Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Execute main training loop\n",
    "\n",
    "training_loop(train_ds, val_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Fine-tune by contrastive learning on objects+texts\n",
    "\n",
    "In this other approach we're gonna:\n",
    "- import the original CLIP, without adding other layers on top\n",
    "- implement the contrastive loss logic and adapt it into the training and test step defined earlier\n",
    "- train the model on all images - caption pairs in the dataset, using the contrastive loss\n",
    "\n",
    "Note that when speaking of images, we do not mean the whole sample images, but the images cropped at the ground truth bbox. In other words, we are training CLIP to maximize the similarity between the embedding of each image of every object refered by the dataset and the corresponding (1 or more) caption/s.\n",
    "\n",
    "This should bring some **benefits**:\n",
    "- CLIP should become *better* at extracting embeddings for the images and captions of RefCOCOg.\n",
    "- [add others, if any]\n",
    "\n",
    "and some **drawbacks** too:\n",
    "- as before, CLIP zero-shot capabilities would be basically lost\n",
    "- [add others, if any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.019106Z",
     "start_time": "2023-04-29T14:10:54.247540Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import CLIP model and show its info\n",
    "\n",
    "clip_model, clip_prep = clip.load(\"RN50\", device=device)\n",
    "\n",
    "print(\"[INFO] Model params: {:,}\".format(np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()])))\n",
    "print(\"[INFO] Trainable params: {:,}\".format(sum(p.numel() for p in clip_model.parameters() if p.requires_grad)))\n",
    "print(\"[INFO] Input resolution: \", clip_model.visual.input_resolution)\n",
    "print(\"[INFO] Max prompt length:\", clip_model.context_length)\n",
    "print(\"[INFO] Vocab size:\", clip_model.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.111368Z",
     "start_time": "2023-04-29T14:10:59.028213Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Contrastive loss function definition\n",
    "\n",
    "def contrastive_loss(image_logits, text_logits, cost_function):\n",
    "    labels = np.arange(image_logits.shape[0])\n",
    "    labels = torch.from_numpy(labels).to(device)\n",
    "\n",
    "    loss_i = cost_function(image_logits, labels)\n",
    "    loss_t = cost_function(text_logits, labels)\n",
    "\n",
    "    return (loss_i + loss_t) / 2.0\n",
    "\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum, optimizer):\n",
    "    try:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.visual.layer4.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd, momentum=momentum)\n",
    "    except TypeError:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.visual.layer4.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd)\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.203471Z",
     "start_time": "2023-04-29T14:10:59.122680Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Slight modifications to the aforementioned logics\n",
    "\n",
    "\n",
    "def training_step_cl(net, data_loader, optimizer, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "\n",
    "    # set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Training step\")):\n",
    "\n",
    "        images, texts = list(), list()\n",
    "\n",
    "        for sample in batch:\n",
    "            sample = RefCOCOgSample(**sample)\n",
    "\n",
    "            for sentence in sample.sentences:\n",
    "                prep_img = sample.img.crop(sample.bbox)\n",
    "                prep_img = clip_prep(prep_img)\n",
    "\n",
    "                images.append(prep_img)\n",
    "                texts.append(sentence)\n",
    "\n",
    "        texts = clip.tokenize(texts).to(device)\n",
    "        images = torch.stack(images).to(device)\n",
    "\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        image_logits, text_logits = net(images, texts)\n",
    "\n",
    "        # loss computation\n",
    "        loss = contrastive_loss(image_logits, text_logits, cost_function)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fetch loss value\n",
    "        n_samples += images.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss / n_samples\n",
    "\n",
    "\n",
    "def test_step_cl(net, data_loader, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Test step\")):\n",
    "\n",
    "            images, texts = list(), list()\n",
    "\n",
    "            for sample in batch:\n",
    "                sample = RefCOCOgSample(**sample)\n",
    "\n",
    "                for sentence in sample.sentences:\n",
    "                    prep_img = sample.img.crop(sample.bbox)\n",
    "                    prep_img = clip_prep(prep_img)\n",
    "\n",
    "                    images.append(prep_img)\n",
    "                    texts.append(sentence)\n",
    "\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "            images = torch.stack(images).to(device)\n",
    "\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            image_logits, text_logits = net(images, texts)\n",
    "\n",
    "            # loss computation\n",
    "            loss = contrastive_loss(image_logits, text_logits, cost_function)\n",
    "\n",
    "            # fetch loss value\n",
    "            n_samples += images.shape[0]\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss / n_samples\n",
    "\n",
    "\n",
    "def log_values_cl(writer, step, loss, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "\n",
    "\n",
    "def main_loop_cl(train_ds,\n",
    "                 val_ds,\n",
    "                 test_ds,\n",
    "                 batch_size=batch_size,\n",
    "                 num_classes=90,  # 90 classes in RefCOCOg\n",
    "                 device=device,\n",
    "                 learning_rate=learning_rate,\n",
    "                 weight_decay=0.000001,\n",
    "                 momentum=momentum,\n",
    "                 epochs=epochs,\n",
    "                 optimizer=optimizer):\n",
    "    # create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "    # instantiate the network and move it to the chosen device (GPU)\n",
    "    net = clip_model.to(device)\n",
    "\n",
    "    # instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum, optimizer)\n",
    "\n",
    "    # define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    # computes evaluation results before training\n",
    "    # TODO: fix error here: CustomCLIP.forward() takes 2 positional arguments but 3 were given\n",
    "    print('Before training:')\n",
    "    train_loss = test_step_cl(net, val_loader, cost_function)\n",
    "    val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "    test_loss = test_step_cl(net, test_loader, cost_function)\n",
    "\n",
    "    # print(train_loss)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values_cl(writer, -1, train_loss, \"train\")\n",
    "    log_values_cl(writer, -1, val_loss, \"validation\")\n",
    "    log_values_cl(writer, -1, test_loss, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "    print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "    print('\\tTest loss {:.5f}'.format(test_loss))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # for each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss = training_step_cl(net, train_loader, optimizer, cost_function)\n",
    "        val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "\n",
    "        # logs to TensorBoard\n",
    "        log_values_cl(writer, e, val_loss, \"Validation\")\n",
    "\n",
    "        print('Epoch: {:d}'.format(e + 1))\n",
    "        print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "        print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "    # compute final evaluation results\n",
    "    print('After training:')\n",
    "    train_loss = test_step_cl(net, train_loader, cost_function)\n",
    "    val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "    test_loss = test_step_cl(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values_cl(writer, epochs, train_loss, \"train\")\n",
    "    log_values_cl(writer, epochs, val_loss, \"validation\")\n",
    "    log_values_cl(writer, epochs, test_loss, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "    print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "    print('\\tTest loss {:.5f}'.format(test_loss))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # closes the logger\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.339097Z",
     "start_time": "2023-04-29T14:10:59.217477Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Execute main training loop\n",
    "\n",
    "main_loop_cl(train_ds, val_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Grounding test\n",
    "\n",
    "The following is to verify the effect of these approaches to the main visual grounding task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.474492Z",
     "start_time": "2023-04-29T14:10:59.373451Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Function definition to test visual grounding with a given pipeline\n",
    "\n",
    "def visual_grounding_test(vg_pipeline, dataset):\n",
    "    scores = list()\n",
    "\n",
    "    for sample in tqdm(dataset, desc=f\"Testing on {len(dataset)} images\"):\n",
    "\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "\n",
    "            try:\n",
    "                sc = vg_pipeline(sample, sentence, show=False)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            scores.append(sc)\n",
    "\n",
    "    for metric in scores[0].keys():\n",
    "        avg_metric = np.mean([score[metric] for score in scores])\n",
    "\n",
    "        print(\"Avg. {}: {:.3f}\".format(metric, avg_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T08:51:26.185676Z",
     "start_time": "2023-04-27T08:37:24.375443Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Import the YoloClip pipeline and test it on the test dataset\n",
    "\n",
    "from modules.yoloclip import YoloClip\n",
    "\n",
    "yoloclip = YoloClip(device=device, quiet=True, categories=dataset.dataset.categories)\n",
    "\n",
    "visual_grounding_test(yoloclip, test_ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
