{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CLIP\n",
    "\n",
    "The following notebook shows two different takes at fine-tuning CLIP.\n",
    "\n",
    "1. Fine tuning CLIP visual backbone on the RefCOCOg dataset (images)\n",
    "2. Fine tuning CLIP performing contrastive learning on the RefCOCOg dataset (text + images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dmmp/.pyenv/shims/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3627058317.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m\u001b[0m\n\u001b[0;31m    jupyter kernelspec list\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter kernelspec list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:26.528476Z",
     "start_time": "2023-04-29T14:10:22.134777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GPU found, using GPU.\n"
     ]
    }
   ],
   "source": [
    "#@title Import necessary packages and set correct device\n",
    "\n",
    "import os\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.refcocog import RefCOCOg, RefCOCOgSample\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"[INFO] GPU found, using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"[INFO] No GPU found, using CPU instead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZERS_TO_TRY = {\n",
    "    \"SGD\":torch.optim.SGD,\n",
    "    \"RMSProp\":torch.optim.RMSprop,\n",
    "    \"Adam\":torch.optim.Adam,\n",
    "    \"Adamax\":torch.optim.Adamax,\n",
    "    \"Adadelta\":torch.optim.Adadelta,\n",
    "    # todo: add more\n",
    "}\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "\n",
    "batch_size = 128 # 256 causes out of memory with 24GB of GPU ram\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "epochs = 10\n",
    "optimizer = \"Adam\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:29.919895Z",
     "start_time": "2023-04-29T14:10:26.461935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model params: 102,007,137\n",
      "[INFO] Trainable params: 102,007,137\n",
      "[INFO] Input resolution:  224\n",
      "[INFO] Max prompt length: 77\n",
      "[INFO] Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "#@title Import CLIP model and show its info\n",
    "\n",
    "clip_model, clip_prep = clip.load(\"RN50\", device=device)\n",
    "\n",
    "print(\"[INFO] Model params: {:,}\".format(np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()])))\n",
    "print(\"[INFO] Trainable params: {:,}\".format(sum(p.numel() for p in clip_model.parameters() if p.requires_grad)))\n",
    "print(\"[INFO] Input resolution: \", clip_model.visual.input_resolution)\n",
    "print(\"[INFO] Max prompt length:\", clip_model.context_length)\n",
    "print(\"[INFO] Vocab size:\", clip_model.vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, solely for debuggining purposes on local machines, we are discarding most of the dataset and using only a \"toy\" portion of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:53.914478Z",
     "start_time": "2023-04-29T14:10:29.736385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 4982\n",
      "\n",
      "Train size: 4222\n",
      "Val size:   257\n",
      "Test size:  502\n"
     ]
    }
   ],
   "source": [
    "#@title Import RefCOCOg dataset and its train/val/test splits\n",
    "\n",
    "# modify\n",
    "dataset_path = \"/media/dmmp/vid+backup/Data/refcocog\"\n",
    "\n",
    "dataset = RefCOCOg(ds_path=dataset_path)\n",
    "\n",
    "train_ds = RefCOCOg(ds_path=dataset_path, split='train')\n",
    "val_ds = RefCOCOg(ds_path=dataset_path, split='val')\n",
    "test_ds = RefCOCOg(ds_path=dataset_path, split='test')\n",
    "\n",
    "keep = 0.1\n",
    "dataset, _ = random_split(dataset, [int(keep * len(dataset)), len(dataset) - int(keep * len(dataset))])\n",
    "train_ds, _ = random_split(train_ds, [int(keep * len(train_ds)), len(train_ds) - int(keep * len(train_ds))])\n",
    "val_ds, _ = random_split(val_ds, [int(keep * len(val_ds)), len(val_ds) - int(keep * len(val_ds))])\n",
    "test_ds, _ = random_split(test_ds, [int(keep * len(test_ds)), len(test_ds) - int(keep * len(test_ds))])\n",
    "\n",
    "print(f\"Dataset Size: {len(dataset)}\\n\")\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Val size:   {len(val_ds)}\")\n",
    "print(f\"Test size:  {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.042399Z",
     "start_time": "2023-04-29T14:10:53.917978Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title An example of computing images-prompts similarity\n",
    "\n",
    "### Useless for the actual training loop, but was in Alessandro's code.\n",
    "### But functions below might be helpful later ¯\\(ツ)/¯\n",
    "\n",
    "def get_data(dataset):\n",
    "    texts, images = list(), list()\n",
    "\n",
    "    for sample in tqdm(dataset, desc=\"[INFO] Loading images and captions\"):\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "            images.append(sample.path)\n",
    "            texts.append(sentence)\n",
    "\n",
    "    return images, texts\n",
    "\n",
    "\n",
    "def encode_data(images_fp: list[str], texts: list[str]):\n",
    "    # preprocess the images to transform from filenames to images to tensors\n",
    "    images = [clip_prep(Image.open(image)) for image in tqdm(images_fp, desc=\"[INFO] Preprocessing images\")]\n",
    "    images = torch.tensor(np.stack(images)).to(device)\n",
    "\n",
    "    # preprocess the texts to transform from text to tensors\n",
    "    text_tokens = clip.tokenize([\"This is \" + desc for desc in tqdm(texts, desc=\"[INFO] Preprocessing texts\")]).to(\n",
    "        device)\n",
    "\n",
    "    # encode the inputs\n",
    "    with torch.no_grad():\n",
    "        print(\"[INFO] Encoding images...\")\n",
    "        images_z = clip_model.encode_image(images).float()\n",
    "        print(\"[INFO] Encoding texts...\")\n",
    "        texts_z = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "    return images_z, texts_z\n",
    "\n",
    "\n",
    "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
    "    # normalise the image and the text\n",
    "    images_z /= images_z.norm(dim=-1, keepdim=True)\n",
    "    texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # evaluate the cosine similarity between the sets of features\n",
    "    similarity = (texts_z @ images_z.T)\n",
    "\n",
    "    return similarity.cpu()\n",
    "\n",
    "# images_fp, texts = get_data(test_ds)\n",
    "\n",
    "# images_z, texts_z = encode_data(images_fp, texts)\n",
    "\n",
    "# similarity = cosine_similarity(images_z, texts_z)\n",
    "\n",
    "# print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune by classifying on RefCOCOg classes\n",
    "\n",
    "In the following approach we're gonna:\n",
    "- create a custom CLIP architecture with an additional trainable layer\n",
    "- implement training and testing logics\n",
    "- train the newly created CLIP model on the images from the whole dataset\n",
    "\n",
    "This should bring some **benefits**:\n",
    "- CLIP should become *better* at extracting features from our RefCOCOg images\n",
    "- [add others, if any]\n",
    "\n",
    "and some **drawbacks** too:\n",
    "- The textual encoding training is basically lost\n",
    "- [add others, if any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.527738Z",
     "start_time": "2023-04-29T14:10:53.978706Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Custom CLIP architecture featuring an additional fc layer\n",
    "\n",
    "class CustomCLIP(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        model, _ = clip.load(\"RN50\")\n",
    "\n",
    "        # take the visual encoder of CLIP\n",
    "        # we also convert it to be 32 bit (by default CLIP is 16)\n",
    "        self.encoder = model.visual.float()\n",
    "\n",
    "        # add a linear layer\n",
    "        self.classifier = torch.nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.544572Z",
     "start_time": "2023-04-29T14:10:54.029665Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Training and test logics\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum, optimizer):\n",
    "    try:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.classifier.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd, momentum=momentum)\n",
    "    except TypeError:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.classifier.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_cost_function():\n",
    "    cost_function = torch.nn.CrossEntropyLoss()\n",
    "    return cost_function\n",
    "\n",
    "\n",
    "def training_step(net, data_loader, optimizer, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    # iterate over the training set\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Training step\")):\n",
    "\n",
    "        inputs, targets = list(), list()\n",
    "\n",
    "        for sample in batch:\n",
    "            sample = RefCOCOgSample(**sample)\n",
    "\n",
    "            prep_img = clip_prep(sample.img)\n",
    "\n",
    "            inputs.append(prep_img)\n",
    "            targets.append(sample.category_id - 1)  # so that category_ids will start from #0\n",
    "\n",
    "        inputs = torch.stack(inputs)\n",
    "        targets = torch.tensor(targets)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # loss computation\n",
    "        loss = cost_function(outputs, targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fetch prediction and loss value\n",
    "        n_samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "        _, predicted = outputs.max(dim=1)  # max() returns (maximum_value, index_of_maximum_value)\n",
    "\n",
    "        # compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / n_samples, cumulative_accuracy / n_samples * 100\n",
    "\n",
    "\n",
    "def test_step(net, data_loader, cost_function, device=device):\n",
    "    samples_ = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
    "    with torch.no_grad():\n",
    "        # iterate over the test set\n",
    "        for batch_idx, samples in enumerate(tqdm(data_loader, desc=\"[INFO] Test step\")):\n",
    "\n",
    "            inputs, targets = list(), list()\n",
    "\n",
    "            for sample in samples:\n",
    "                sample = RefCOCOgSample(**sample)\n",
    "\n",
    "                prep_img = clip_prep(sample.img)\n",
    "\n",
    "                inputs.append(prep_img)\n",
    "                targets.append(sample.category_id - 1)  # so that category_ids will start from #0\n",
    "\n",
    "            inputs = torch.stack(inputs)\n",
    "            targets = torch.tensor(targets)\n",
    "\n",
    "            # load data into GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # loss computation\n",
    "            loss = cost_function(outputs, targets)\n",
    "\n",
    "            # fetch prediction and loss value\n",
    "            samples_ += inputs.shape[0]\n",
    "            cumulative_loss += loss.item()  # Note: the .item() is needed to extract scalars from tensors\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # compute accuracy\n",
    "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return cumulative_loss / samples_, cumulative_accuracy / samples_ * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.622225Z",
     "start_time": "2023-04-29T14:10:54.122760Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Main training loop logic\n",
    "\n",
    "# tensorboard logging utilities\n",
    "def log_values(writer, step, loss, accuracy, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "    writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "\n",
    "def training_loop(train_ds,\n",
    "         val_ds,\n",
    "         test_ds,\n",
    "         batch_size=batch_size,\n",
    "         num_classes=90,  # 90 classes in RefCOCOg\n",
    "         device=device,\n",
    "         learning_rate=learning_rate,\n",
    "         weight_decay=0.000001,\n",
    "         momentum=momentum,\n",
    "         epochs=epochs,\n",
    "         optimizer = optimizer):\n",
    "    # create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "    # instantiate the network and move it to the chosen device (GPU)\n",
    "    net = CustomCLIP(num_classes=num_classes).to(device)\n",
    "\n",
    "    # instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum, optimizer)\n",
    "\n",
    "    # define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    # computes evaluation results before training\n",
    "    print('Before training:')\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, -1, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, -1, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, -1, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # for each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function)\n",
    "        val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "\n",
    "        # logs to TensorBoard\n",
    "        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
    "\n",
    "        print('Epoch: {:d}'.format(e + 1))\n",
    "        print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "        print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "    # compute final evaluation results\n",
    "    print('After training:')\n",
    "    train_loss, train_accuracy = test_step(net, train_loader, cost_function)\n",
    "    val_loss, val_accuracy = test_step(net, val_loader, cost_function)\n",
    "    test_loss, test_accuracy = test_step(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values(writer, epochs, train_loss, train_accuracy, \"train\")\n",
    "    log_values(writer, epochs, val_loss, val_accuracy, \"validation\")\n",
    "    log_values(writer, epochs, test_loss, test_accuracy, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('\\tValidation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
    "    print('\\tTest loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # closes the logger\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:54.623506Z",
     "start_time": "2023-04-29T14:10:54.174153Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_loop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Execute main training loop\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtraining_loop\u001b[49m(train_ds, val_ds, test_ds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_loop' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Execute main training loop\n",
    "\n",
    "training_loop(train_ds, val_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Fine-tune by contrastive learning on objects+texts\n",
    "\n",
    "In this other approach we're gonna:\n",
    "- import the original CLIP, without adding other layers on top\n",
    "- implement the contrastive loss logic and adapt it into the training and test step defined earlier\n",
    "- train the model on all images - caption pairs in the dataset, using the contrastive loss\n",
    "\n",
    "Note that when speaking of images, we do not mean the whole sample images, but the images cropped at the ground truth bbox. In other words, we are training CLIP to maximize the similarity between the embedding of each image of every object refered by the dataset and the corresponding (1 or more) caption/s.\n",
    "\n",
    "This should bring some **benefits**:\n",
    "- CLIP should become *better* at extracting embeddings for the images and captions of RefCOCOg.\n",
    "- [add others, if any]\n",
    "\n",
    "and some **drawbacks** too:\n",
    "- as before, CLIP zero-shot capabilities would be basically lost\n",
    "- [add others, if any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.019106Z",
     "start_time": "2023-04-29T14:10:54.247540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model params: 102,007,137\n",
      "[INFO] Trainable params: 102,007,137\n",
      "[INFO] Input resolution:  224\n",
      "[INFO] Max prompt length: 77\n",
      "[INFO] Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "#@title Import CLIP model and show its info\n",
    "\n",
    "clip_model, clip_prep = clip.load(\"RN50\", device=device)\n",
    "\n",
    "print(\"[INFO] Model params: {:,}\".format(np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()])))\n",
    "print(\"[INFO] Trainable params: {:,}\".format(sum(p.numel() for p in clip_model.parameters() if p.requires_grad)))\n",
    "print(\"[INFO] Input resolution: \", clip_model.visual.input_resolution)\n",
    "print(\"[INFO] Max prompt length:\", clip_model.context_length)\n",
    "print(\"[INFO] Vocab size:\", clip_model.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.111368Z",
     "start_time": "2023-04-29T14:10:59.028213Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Contrastive loss function definition\n",
    "\n",
    "def contrastive_loss(image_logits, text_logits, cost_function):\n",
    "    labels = np.arange(image_logits.shape[0])\n",
    "    labels = torch.from_numpy(labels).to(device)\n",
    "\n",
    "    loss_i = cost_function(image_logits, labels)\n",
    "    loss_t = cost_function(text_logits, labels)\n",
    "\n",
    "    return (loss_i + loss_t) / 2.0\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum, optimizer):\n",
    "    try:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.visual.layer4.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd, momentum=momentum)\n",
    "    except TypeError:\n",
    "        optimizer = OPTIMIZERS_TO_TRY[optimizer]([\n",
    "            {'params': model.visual.layer4.parameters(), 'lr': lr}\n",
    "        ], lr=lr, weight_decay=wd)\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.203471Z",
     "start_time": "2023-04-29T14:10:59.122680Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Slight modifications to the aforementioned logics\n",
    "\n",
    "\n",
    "def training_step_cl(net, data_loader, optimizer, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "\n",
    "    # set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Training step\")):\n",
    "\n",
    "        images, texts = list(), list()\n",
    "\n",
    "        for sample in batch:\n",
    "            sample = RefCOCOgSample(**sample)\n",
    "\n",
    "            for sentence in sample.sentences:\n",
    "                prep_img = sample.img.crop(sample.bbox)\n",
    "                prep_img = clip_prep(prep_img)\n",
    "\n",
    "                images.append(prep_img)\n",
    "                texts.append(sentence)\n",
    "\n",
    "        texts = clip.tokenize(texts).to(device)\n",
    "        images = torch.stack(images).to(device)\n",
    "\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        image_logits, text_logits = net(images, texts)\n",
    "\n",
    "        # loss computation\n",
    "        loss = contrastive_loss(image_logits, text_logits, cost_function)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # fetch loss value\n",
    "        n_samples += images.shape[0]\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss / n_samples\n",
    "\n",
    "\n",
    "def test_step_cl(net, data_loader, cost_function, device=device):\n",
    "    n_samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "\n",
    "    # set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"[INFO] Test step\")):\n",
    "\n",
    "            images, texts = list(), list()\n",
    "\n",
    "            for sample in batch:\n",
    "                sample = RefCOCOgSample(**sample)\n",
    "\n",
    "                for sentence in sample.sentences:\n",
    "                    prep_img = sample.img.crop(sample.bbox)\n",
    "                    prep_img = clip_prep(prep_img)\n",
    "\n",
    "                    images.append(prep_img)\n",
    "                    texts.append(sentence)\n",
    "\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "            images = torch.stack(images).to(device)\n",
    "\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            image_logits, text_logits = net(images, texts)\n",
    "\n",
    "            # loss computation\n",
    "            loss = contrastive_loss(image_logits, text_logits, cost_function)\n",
    "\n",
    "            # fetch loss value\n",
    "            n_samples += images.shape[0]\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss / n_samples\n",
    "\n",
    "\n",
    "def log_values_cl(writer, step, loss, prefix):\n",
    "    writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "\n",
    "\n",
    "def main_loop_cl(train_ds,\n",
    "                 val_ds,\n",
    "                 test_ds,\n",
    "                 batch_size=batch_size,\n",
    "                 num_classes=90,  # 90 classes in RefCOCOg\n",
    "                 device=device,\n",
    "                 learning_rate=learning_rate,\n",
    "                 weight_decay=0.000001,\n",
    "                 momentum=momentum,\n",
    "                 epochs=epochs,\n",
    "                 optimizer=optimizer):\n",
    "    # create a logger for the experiment\n",
    "    writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "    # instantiate the network and move it to the chosen device (GPU)\n",
    "    net = clip_model.to(device)\n",
    "\n",
    "    # instantiate the optimizer\n",
    "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum, optimizer)\n",
    "\n",
    "    # define the cost function\n",
    "    cost_function = get_cost_function()\n",
    "\n",
    "    # computes evaluation results before training\n",
    "    # tODO: fix error here: CustomCLIP.forward() takes 2 positional arguments but 3 were given\n",
    "    print('Before training:')\n",
    "    train_loss = test_step_cl(net, val_loader, cost_function)\n",
    "    val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "    test_loss = test_step_cl(net, test_loader, cost_function)\n",
    "    \n",
    "    # print(train_loss)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values_cl(writer, -1, train_loss, \"train\")\n",
    "    log_values_cl(writer, -1, val_loss, \"validation\")\n",
    "    log_values_cl(writer, -1, test_loss, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "    print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "    print('\\tTest loss {:.5f}'.format(test_loss))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # for each epoch, train the network and then compute evaluation results\n",
    "    for e in range(epochs):\n",
    "        train_loss = training_step_cl(net, train_loader, optimizer, cost_function)\n",
    "        val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "\n",
    "        # logs to TensorBoard\n",
    "        log_values_cl(writer, e, val_loss, \"Validation\")\n",
    "\n",
    "        print('Epoch: {:d}'.format(e + 1))\n",
    "        print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "        print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "    # compute final evaluation results\n",
    "    print('After training:')\n",
    "    train_loss = test_step_cl(net, train_loader, cost_function)\n",
    "    val_loss = test_step_cl(net, val_loader, cost_function)\n",
    "    test_loss = test_step_cl(net, test_loader, cost_function)\n",
    "\n",
    "    # log to TensorBoard\n",
    "    log_values_cl(writer, epochs, train_loss, \"train\")\n",
    "    log_values_cl(writer, epochs, val_loss, \"validation\")\n",
    "    log_values_cl(writer, epochs, test_loss, \"test\")\n",
    "\n",
    "    print('\\tTraining loss {:.5f}'.format(train_loss))\n",
    "    print('\\tValidation loss {:.5f}'.format(val_loss))\n",
    "    print('\\tTest loss {:.5f}'.format(test_loss))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "    # closes the logger\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.339097Z",
     "start_time": "2023-04-29T14:10:59.217477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:09<00:00,  3.19s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 3/3 [00:04<00:00,  1.43s/it]\n",
      "[INFO] Test step: 100%|███████████████████████████| 4/4 [00:15<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss 0.01374\n",
      "\tValidation loss 0.01470\n",
      "\tTest loss 0.01168\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training step:  24%|█████▎                | 8/33 [00:37<01:57,  4.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Execute main training loop\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmain_loop_cl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mmain_loop_cl\u001b[0;34m(train_ds, val_ds, test_ds, batch_size, num_classes, device, learning_rate, weight_decay, momentum, epochs, optimizer)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# for each epoch, train the network and then compute evaluation results\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 147\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step_cl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m test_step_cl(net, val_loader, cost_function)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# logs to TensorBoard\u001b[39;00m\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mtraining_step_cl\u001b[0;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# set the network to training mode\u001b[39;00m\n\u001b[1;32m      9\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training step\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m     13\u001b[0m     images, texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/DeepLearningLab/visual-grounding/modules/refcocog.py:98\u001b[0m, in \u001b[0;36mRefCOCOg.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     91\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds_path,\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_[0-9]+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, refs_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# \"_\".join(ann_data[\"file_name\"].split(\"_\")[:-1])+\".jpg\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m )\n\u001b[0;32m---> 98\u001b[0m pil_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ann_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    101\u001b[0m bbox \u001b[38;5;241m=\u001b[39m box_convert(bbox, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxywh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxyxy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3245\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3242\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3243\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3245\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3247\u001b[0m preinit()\n\u001b[1;32m   3249\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Execute main training loop\n",
    "\n",
    "main_loop_cl(train_ds, val_ds, test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Grounding test\n",
    "\n",
    "The following is to verify the effect of these approaches to the main visual grounding task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T14:10:59.474492Z",
     "start_time": "2023-04-29T14:10:59.373451Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Function definition to test visual grounding with a given pipeline\n",
    "\n",
    "def visual_grounding_test(vg_pipeline, dataset):\n",
    "    scores = list()\n",
    "\n",
    "    for sample in tqdm(dataset, desc=f\"Testing on {len(dataset)} images\"):\n",
    "\n",
    "        sample = RefCOCOgSample(**sample)\n",
    "\n",
    "        for sentence in sample.sentences:\n",
    "\n",
    "            try:\n",
    "                sc = vg_pipeline(sample, sentence, show=False)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            scores.append(sc)\n",
    "\n",
    "    for metric in scores[0].keys():\n",
    "        avg_metric = np.mean([score[metric] for score in scores])\n",
    "\n",
    "        print(\"Avg. {}: {:.3f}\".format(metric, avg_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T08:51:26.185676Z",
     "start_time": "2023-04-27T08:37:24.375443Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /home/dmmp/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /home/dmmp/.cache/torch/hub/requirements.txt not found, check failed.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid CUDA '--device 0' requested, use '--device cpu' or pass valid CUDA device(s). Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:46\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained \u001b[38;5;129;01mand\u001b[39;00m channels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m80\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/utils/torch_utils.py:118\u001b[0m, in \u001b[0;36mselect_device\u001b[0;34m(device, batch_size, newline)\u001b[0m\n\u001b[1;32m    117\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m device  \u001b[38;5;66;03m# set environment variable - must be before assert is_available()\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(device\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)), \\\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requested, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--device cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or pass valid CUDA device(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mps \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# prefer GPU if available\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Invalid CUDA '--device 0' requested, use '--device cpu' or pass valid CUDA device(s)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Import the YoloClip pipeline and test it on the test dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myoloclip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YoloClip\n\u001b[0;32m----> 5\u001b[0m yoloclip \u001b[38;5;241m=\u001b[39m \u001b[43mYoloClip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m visual_grounding_test(yoloclip, test_ds)\n",
      "File \u001b[0;32m~/DeepLearningLab/visual-grounding/modules/yoloclip.py:60\u001b[0m, in \u001b[0;36mYoloClip.__init__\u001b[0;34m(self, categories, yolo_ver, clip_ver, device, quiet, dist_metric)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m              categories,\n\u001b[1;32m     54\u001b[0m              yolo_ver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov5s\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m              quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m              dist_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myolo_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics/yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_prep \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(clip_ver, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquiet \u001b[38;5;241m=\u001b[39m quiet\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/hub.py:558\u001b[0m, in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    555\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    556\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[0;32m--> 558\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/hub.py:587\u001b[0m, in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[1;32m    586\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m--> 587\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:93\u001b[0m, in \u001b[0;36myolov5s\u001b[0;34m(pretrained, channels, classes, autoshape, _verbose, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myolov5s\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# YOLOv5-small model https://github.com/ultralytics/yolov5\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov5s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/hubconf.py:78\u001b[0m, in \u001b[0;36m_create\u001b[0;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m help_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     77\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for help.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Invalid CUDA '--device 0' requested, use '--device cpu' or pass valid CUDA device(s). Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
     ]
    }
   ],
   "source": [
    "#@title Import the YoloClip pipeline and test it on the test dataset\n",
    "\n",
    "from modules.yoloclip import YoloClip\n",
    "\n",
    "yoloclip = YoloClip(device=device, quiet=True, categories=dataset.dataset.categories)\n",
    "\n",
    "visual_grounding_test(yoloclip, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
